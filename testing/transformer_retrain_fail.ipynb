{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"16d_jjK_8VfqFw6o3drQffUsX8S67Pic1","timestamp":1744344698703}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# Install necessary packages\n","!pip install ftfy regex tqdm\n","!pip install git+https://github.com/openai/CLIP.git\n","!pip install scipy\n","\n","import clip\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","from torchvision import transforms\n","from PIL import Image\n","import os\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","import shutil"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lHMXkYSba1KH","outputId":"fc6ac932-32fd-4a0a-8d86-35791711d513","executionInfo":{"status":"ok","timestamp":1744343893926,"user_tz":240,"elapsed":10049,"user":{"displayName":"Ava Collier","userId":"11682034821230281817"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (6.3.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-chkumfvc\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-chkumfvc\n","  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.14.1)\n","Requirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (2.0.2)\n"]}]},{"cell_type":"code","source":["# Check PyTorch version\n","import packaging\n","version = packaging.version.parse(torch.__version__)\n","if version > packaging.version.parse('1.7.0'):\n","    print(\"Pytorch version is above 1.7.0\")\n","    print(\"It is version:\", version)\n","else:\n","    print(\"PyTorch version is not above 1.7.0. Please Upgrade\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Am9l2tDXa5vT","outputId":"1abbbc8f-25f4-4182-e614-237e34224046","executionInfo":{"status":"ok","timestamp":1744343920043,"user_tz":240,"elapsed":11,"user":{"displayName":"Ava Collier","userId":"11682034821230281817"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pytorch version is above 1.7.0\n","It is version: 2.6.0+cu124\n"]}]},{"cell_type":"code","source":["# Load CLIP model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)"],"metadata":{"id":"UPvdMxm4a8mr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Unfreeze more layers from CLIP\n","for name, param in model.named_parameters():\n","    if \"visual\" in name:\n","        param.requires_grad = True"],"metadata":{"id":"ol9ZYKNra_mc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TransformerHead(nn.Module):\n","    def __init__(self, input_dim, output_dim=512, num_heads=8, num_layers=2, dropout_prob=0.2):\n","        super(TransformerHead, self).__init__()\n","        self.attn_layer = nn.MultiheadAttention(embed_dim=input_dim, num_heads=num_heads, batch_first=True)\n","        self.transformer = nn.TransformerEncoder(\n","            nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dropout=dropout_prob),\n","            num_layers=num_layers\n","        )\n","        self.fc_img = nn.Linear(input_dim, output_dim)\n","        self.fc_txt = nn.Linear(input_dim, output_dim)\n","        self.layer_norm = nn.LayerNorm(input_dim)\n","        self.dropout = nn.Dropout(dropout_prob)\n","\n","    def forward(self, image_features, text_features):\n","        features = torch.cat((image_features, text_features), dim=-1)  # Concatenate image + text features\n","        features = features.unsqueeze(1)  # Add batch dimension for attention\n","        attn_output, _ = self.attn_layer(features, features, features)  # Attention layer\n","        attn_output = self.dropout(attn_output)  # Apply dropout to attention output\n","        transformer_output = self.transformer(attn_output)  # Transformer layer\n","        transformer_output = transformer_output.squeeze(1)  # Remove sequence length dimension\n","\n","        transformer_output = self.dropout(transformer_output)  # Dropout applied on the transformer output\n","        img_out = self.fc_img(transformer_output)  # Image branch\n","        txt_out = self.fc_txt(transformer_output)  # Text branch\n","        return img_out, txt_out\n","\n","    def encode_image_only(self, image_features):\n","        # Combine with dummy text input\n","        dummy_text = torch.zeros_like(image_features)  # (B, 512)\n","        features = torch.cat((image_features, dummy_text), dim=-1)  # (B, 1024)\n","        features = features.unsqueeze(1)  # (B, 1, 1024)\n","\n","        # Pass through attention and transformer\n","        attn_output, _ = self.attn_layer(features, features, features)\n","        attn_output = self.dropout(attn_output)  # Apply dropout to attention output\n","        transformer_output = self.transformer(attn_output).squeeze(1)\n","        transformer_output = self.layer_norm(transformer_output)  # (B, 1024)\n","\n","        # Output 512-dim embedding from the image branch\n","        img_out = self.fc_img(transformer_output)  # (B, 512)\n","        return img_out\n"],"metadata":{"id":"hiaZm425bZFU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# DATA HERE!!!!"],"metadata":{"id":"MqGkpdBUqLKo"}},{"cell_type":"code","source":["import kagglehub\n","\n","# Download latest version\n","path = kagglehub.dataset_download(\"amirmakir/dogs-dataset\")\n","\n","print(\"Path to dataset files:\", path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zqOiUSclqNXA","executionInfo":{"status":"ok","timestamp":1744342633561,"user_tz":240,"elapsed":2723,"user":{"displayName":"Ava Collier","userId":"11682034821230281817"}},"outputId":"7531a9dc-605e-48ce-ab04-bfb8bcf07dda"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Path to dataset files: /kaggle/input/dogs-dataset\n"]}]},{"cell_type":"code","source":["dataset_root = os.path.join(path, 'dogs', 'train')"],"metadata":{"id":"wcIvXdLjqRmA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import shutil\n","from sklearn.model_selection import train_test_split\n","\n","# Define paths\n","dataset_root = os.path.join(path, 'dogs', 'train')  # Path to your \"train\" folder\n","val_root = \"/kaggle/working/dogs/val\"  # Path to save the validation split (working directory)\n","\n","# Create the validation root folder if it doesn't exist\n","if not os.path.exists(val_root):\n","    os.makedirs(val_root)\n","\n","# Split data within each class folder\n","for class_name in os.listdir(dataset_root):\n","    class_folder = os.path.join(dataset_root, class_name)\n","    if os.path.isdir(class_folder):\n","        # Get list of image files in the class folder\n","        image_files = [f for f in os.listdir(class_folder) if os.path.isfile(os.path.join(class_folder, f))]\n","\n","        # Split the images into training and validation sets\n","        train_files, val_files = train_test_split(image_files, test_size=0.2, random_state=42)\n","\n","        # Create corresponding folders in the validation directory\n","        val_class_folder = os.path.join(val_root, class_name)\n","        if not os.path.exists(val_class_folder):\n","            os.makedirs(val_class_folder)\n","\n","        # Copy validation images to the validation folder\n","        for val_image in val_files:\n","            src = os.path.join(class_folder, val_image)\n","            dst = os.path.join(val_class_folder, val_image)\n","            shutil.copy(src, dst)  # Use copy instead of move\n","\n","# After this, the validation set should be created in /kaggle/working/dogs/val\n"],"metadata":{"id":"21sGP7OGwU-M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision import datasets\n","from torch.utils.data import DataLoader\n","import clip\n","import os\n","\n","# Define the transformation for CLIP preprocessing (same as when we loaded the model)\n","# CLIP preprocess automatically resizes, normalizes, and converts to tensor\n","train_transform = preprocess\n","val_transform = preprocess\n","\n","# Create datasets for train and validation using ImageFolder\n","train_dataset = datasets.ImageFolder(root=dataset_root, transform=train_transform)\n","val_dataset = datasets.ImageFolder(root=val_root, transform=val_transform)\n","\n","# Create DataLoaders for train and validation sets\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"],"metadata":{"id":"HOFSEO2hqXyo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class_names = train_dataset.classes\n","print(class_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"My0rOfwSrP6h","executionInfo":{"status":"ok","timestamp":1744344078092,"user_tz":240,"elapsed":17,"user":{"displayName":"Ava Collier","userId":"11682034821230281817"}},"outputId":"bbc4692b-d7f4-4994-be6e-804af60687cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Afghan_hound', 'Blenheim_spaniel', 'Chihuahua', 'Japanese_spaniel', 'Maltese_dog', 'Pekinese', 'Rhodesian_ridgeback', 'Shih_Tzu', 'papillon', 'toy_terrier']\n"]}]},{"cell_type":"code","source":["def generate_text_prompts(labels, class_names):\n","    return [f\"A photo of a {class_names[label]}\" for label in labels]"],"metadata":{"id":"NkxtCwX6rSYl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_dim = 512 * 2\n","transformer_head = TransformerHead(input_dim=input_dim, output_dim=512).to(device)"],"metadata":{"id":"g5SYHsGfbcey"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize optimizer, scaler, and scheduler\n","optimizer = torch.optim.AdamW(transformer_head.parameters(), lr=1e-6, weight_decay=1e-4)\n","scaler = torch.cuda.amp.GradScaler(enabled=True)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-6)\n","\n","# CLIP loss function\n","def clip_loss(image_features, text_features):\n","    logits_per_image = image_features @ text_features.T\n","    logits_per_text = logits_per_image.T\n","    labels = torch.arange(image_features.size(0), device=image_features.device)\n","    loss_img = F.cross_entropy(logits_per_image, labels)\n","    loss_txt = F.cross_entropy(logits_per_text, labels)\n","    return (loss_img + loss_txt) / 2\n","\n","# Training loop with early stopping and learning rate scheduler\n","num_epochs = 10\n","best_val_loss = float('inf')\n","patience = 5\n","epochs_without_improvement = 0\n","\n","for epoch in range(num_epochs):\n","    transformer_head.train()\n","    running_train_loss = 0.0\n","\n","    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Train)\"):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        texts = [f\"A photo of a {train_dataset.classes[label]}\" for label in labels]\n","        texts = clip.tokenize(texts).to(device)\n","\n","        with torch.cuda.amp.autocast():\n","            image_features = model.encode_image(images)\n","            text_features = model.encode_text(texts)\n","            img_out, txt_out = transformer_head(image_features, text_features)\n","            loss = clip_loss(img_out, txt_out)\n","\n","        optimizer.zero_grad()\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        running_train_loss += loss.item()\n","\n","    avg_train_loss = running_train_loss / len(train_loader)\n","    print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {avg_train_loss:.4f}\")\n","\n","    # Validation phase\n","    transformer_head.eval()\n","    running_val_loss = 0.0\n","\n","    with torch.no_grad():\n","        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Val)\"):\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            texts = [f\"A photo of a {train_dataset.classes[label]}\" for label in labels]\n","            texts = clip.tokenize(texts).to(device)\n","\n","            image_features = model.encode_image(images)\n","            text_features = model.encode_text(texts)\n","            loss = clip_loss(image_features, text_features)\n","            running_val_loss += loss.item()\n","\n","    avg_val_loss = running_val_loss / len(val_loader)\n","    print(f\"Epoch {epoch+1}/{num_epochs} - Validation Loss: {avg_val_loss:.4f}\")\n","\n","    if avg_val_loss < best_val_loss:\n","        best_val_loss = avg_val_loss\n","        epochs_without_improvement = 0\n","        print(f\"Validation loss improved. Saving model...\")\n","        torch.save(transformer_head.state_dict(), 'best_transformer_head.pth')\n","    else:\n","        epochs_without_improvement += 1\n","\n","    if epochs_without_improvement >= patience:\n","        print(\"Early stopping triggered. Training stopped.\")\n","        break\n","\n","    scheduler.step()\n","\n","# Load best model and save the fine-tuned model\n","transformer_head.load_state_dict(torch.load('best_transformer_head.pth'))\n","torch.save(transformer_head.state_dict(), 'transformer_head_finetuned.pth')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6O2clmHwvIkl","executionInfo":{"status":"ok","timestamp":1744344201927,"user_tz":240,"elapsed":116313,"user":{"displayName":"Ava Collier","userId":"11682034821230281817"}},"outputId":"e0b7175b-7bd7-47ff-e65d-fd85e796a2f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-205-bfd7b794626d>:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=True)\n","Epoch 1/10 (Train):   0%|          | 0/51 [00:00<?, ?it/s]<ipython-input-205-bfd7b794626d>:31: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Epoch 1/10 (Train): 100%|██████████| 51/51 [00:17<00:00,  2.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10 - Training Loss: 12.6524\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/10 (Val): 100%|██████████| 11/11 [00:02<00:00,  4.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10 - Validation Loss: 3.9354\n","Validation loss improved. Saving model...\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/10 (Train): 100%|██████████| 51/51 [00:17<00:00,  2.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/10 - Training Loss: 11.2631\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/10 (Val): 100%|██████████| 11/11 [00:02<00:00,  5.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/10 - Validation Loss: 3.9354\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/10 (Train): 100%|██████████| 51/51 [00:16<00:00,  3.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/10 - Training Loss: 11.0106\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/10 (Val): 100%|██████████| 11/11 [00:02<00:00,  5.42it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/10 - Validation Loss: 3.9354\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/10 (Train): 100%|██████████| 51/51 [00:17<00:00,  2.98it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4/10 - Training Loss: 9.8913\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/10 (Val): 100%|██████████| 11/11 [00:02<00:00,  5.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4/10 - Validation Loss: 3.9354\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/10 (Train): 100%|██████████| 51/51 [00:16<00:00,  3.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5/10 - Training Loss: 9.3664\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/10 (Val): 100%|██████████| 11/11 [00:02<00:00,  3.93it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5/10 - Validation Loss: 3.9354\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/10 (Train): 100%|██████████| 51/51 [00:16<00:00,  3.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6/10 - Training Loss: 8.4260\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/10 (Val): 100%|██████████| 11/11 [00:02<00:00,  5.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6/10 - Validation Loss: 3.9354\n","Early stopping triggered. Training stopped.\n"]}]},{"cell_type":"markdown","source":["## Compute Accuracy with Newly Trained Model"],"metadata":{"id":"wDH6f6RZ1HGc"}},{"cell_type":"code","source":["from torchvision import datasets\n","from torch.utils.data import DataLoader\n","\n","# Paths\n","test_root = os.path.join(path, 'dogs', 'test')\n","\n","# Load test set\n","test_dataset = datasets.ImageFolder(root=test_root, transform=preprocess)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","# Get all class names\n","class_names = test_dataset.classes\n","print(\"Class names:\", class_names)\n","\n","# Generate text features for all classes once\n","with torch.no_grad():\n","    all_texts = [f\"A photo of a {classname}\" for classname in class_names]\n","    tokenized_texts = clip.tokenize(all_texts).to(device)\n","    text_features_all = model.encode_text(tokenized_texts)  # Shape: (num_classes, 512)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fnrj6gty0xbm","executionInfo":{"status":"ok","timestamp":1744344204700,"user_tz":240,"elapsed":36,"user":{"displayName":"Ava Collier","userId":"11682034821230281817"}},"outputId":"f0b33b9a-f3d4-4617-db9f-35f026239a8d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Class names: ['Afghan_hound', 'Blenheim_spaniel', 'Chihuahua', 'Japanese_spaniel', 'Maltese_dog', 'Pekinese', 'Rhodesian_ridgeback', 'Shih_Tzu', 'papillon', 'toy_terrier']\n"]}]},{"cell_type":"code","source":["def compute_topk_accuracy(image_features, text_features_all, labels, topk=(1, 3, 5)):\n","    # Compute similarity scores\n","    logits = image_features @ text_features_all.T  # (B, num_classes)\n","    _, topk_indices = logits.topk(max(topk), dim=1, largest=True, sorted=True)\n","\n","    results = {}\n","    for k in topk:\n","        correct = topk_indices[:, :k].eq(labels.view(-1, 1)).sum().item()\n","        results[f\"top{k}\"] = correct\n","    return results"],"metadata":{"id":"pP5VCjNd1Y0C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Normalize text features for cosine similarity (ensure both are float32)\n","text_features_all_norm = F.normalize(text_features_all, dim=-1).float()\n","\n","with torch.no_grad():\n","    for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        # Get image features and cast to float32\n","        image_features = model.encode_image(images).float()\n","\n","        # Get 512-dim image embedding from your transformer\n","        img_out = transformer_head.encode_image_only(image_features)\n","\n","        # Normalize both image and text features\n","        img_out = F.normalize(img_out, dim=-1)\n","        text_features_all_norm = F.normalize(text_features_all, dim=-1).float()  # Ensures dtype match\n","\n","        # Compare embeddings via dot product (cosine sim)\n","        accs = compute_topk_accuracy(img_out, text_features_all_norm, labels)\n","\n","        print(\"img_out shape:\", img_out.shape)  # Should be (B, 512)\n","        print(\"text_features_all shape:\", text_features_all.shape)  # Should be (num_classes, 512)\n","\n","        top1_total += accs['top1']\n","        top3_total += accs['top3']\n","        top5_total += accs['top5']\n","        num_samples += images.size(0)\n","\n","# Final results\n","print(f\"\\nTop-1 Accuracy: {top1_total / num_samples * 100:.2f}%\")\n","print(f\"Top-3 Accuracy: {top3_total / num_samples * 100:.2f}%\")\n","print(f\"Top-5 Accuracy: {top5_total / num_samples * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PqmSJM0h1bqP","executionInfo":{"status":"ok","timestamp":1744344210426,"user_tz":240,"elapsed":2194,"user":{"displayName":"Ava Collier","userId":"11682034821230281817"}},"outputId":"c1e73ccf-1634-4f95-fcb0-fdfa0b5da38b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Evaluating:  10%|█         | 1/10 [00:00<00:02,  4.17it/s]"]},{"output_type":"stream","name":"stdout","text":["img_out shape: torch.Size([32, 512])\n","text_features_all shape: torch.Size([10, 512])\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluating:  20%|██        | 2/10 [00:00<00:01,  4.14it/s]"]},{"output_type":"stream","name":"stdout","text":["img_out shape: torch.Size([32, 512])\n","text_features_all shape: torch.Size([10, 512])\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluating:  30%|███       | 3/10 [00:00<00:01,  4.40it/s]"]},{"output_type":"stream","name":"stdout","text":["img_out shape: torch.Size([32, 512])\n","text_features_all shape: torch.Size([10, 512])\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluating:  40%|████      | 4/10 [00:00<00:01,  4.57it/s]"]},{"output_type":"stream","name":"stdout","text":["img_out shape: torch.Size([32, 512])\n","text_features_all shape: torch.Size([10, 512])\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluating:  50%|█████     | 5/10 [00:01<00:01,  4.65it/s]"]},{"output_type":"stream","name":"stdout","text":["img_out shape: torch.Size([32, 512])\n","text_features_all shape: torch.Size([10, 512])\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluating:  60%|██████    | 6/10 [00:01<00:00,  4.73it/s]"]},{"output_type":"stream","name":"stdout","text":["img_out shape: torch.Size([32, 512])\n","text_features_all shape: torch.Size([10, 512])\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluating:  70%|███████   | 7/10 [00:01<00:00,  4.68it/s]"]},{"output_type":"stream","name":"stdout","text":["img_out shape: torch.Size([32, 512])\n","text_features_all shape: torch.Size([10, 512])\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating:  90%|█████████ | 9/10 [00:01<00:00,  4.83it/s]"]},{"output_type":"stream","name":"stdout","text":["img_out shape: torch.Size([32, 512])\n","text_features_all shape: torch.Size([10, 512])\n","img_out shape: torch.Size([32, 512])\n","text_features_all shape: torch.Size([10, 512])\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 10/10 [00:02<00:00,  4.57it/s]"]},{"output_type":"stream","name":"stdout","text":["img_out shape: torch.Size([29, 512])\n","text_features_all shape: torch.Size([10, 512])\n","\n","Top-1 Accuracy: 12.18%\n","Top-3 Accuracy: 32.30%\n","Top-5 Accuracy: 54.20%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["def compute_zero_shot_accuracy(model, image_loader, text_features_all, device, topk=(1, 3, 5)):\n","    top1_total = 0\n","    top3_total = 0\n","    top5_total = 0\n","    num_samples = 0\n","\n","    # Ensure text features are normalized\n","    text_features_all_norm = F.normalize(text_features_all, dim=-1).float()\n","\n","    with torch.no_grad():\n","        for images, labels in tqdm(image_loader, desc=\"Zero-shot Evaluation\"):\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            # Get image features (using zero-shot CLIP model)\n","            image_features = model.encode_image(images).float()\n","\n","            # Normalize the image features\n","            image_features = F.normalize(image_features, dim=-1)\n","\n","            # Compute cosine similarity between image features and text features\n","            logits = image_features @ text_features_all_norm.T  # (B, num_classes)\n","            _, topk_indices = logits.topk(max(topk), dim=1, largest=True, sorted=True)\n","\n","            # Compute the top-k accuracy\n","            for k in topk:\n","                correct = topk_indices[:, :k].eq(labels.view(-1, 1)).sum().item()\n","                if k == 1:\n","                    top1_total += correct\n","                elif k == 3:\n","                    top3_total += correct\n","                elif k == 5:\n","                    top5_total += correct\n","            num_samples += images.size(0)\n","\n","    top1_accuracy = top1_total / num_samples * 100\n","    top3_accuracy = top3_total / num_samples * 100\n","    top5_accuracy = top5_total / num_samples * 100\n","\n","    return top1_accuracy, top3_accuracy, top5_accuracy\n"],"metadata":{"id":"JxD1nMpi4cJR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load text features for all classes once\n","with torch.no_grad():\n","    all_texts = [f\"A photo of a {classname}\" for classname in class_names]\n","    tokenized_texts = clip.tokenize(all_texts).to(device)\n","    text_features_all = model.encode_text(tokenized_texts)  # Shape: (num_classes, 512)\n","\n","# Compute zero-shot accuracy on the test set using the zero-shot CLIP model\n","top1_zero_shot, top3_zero_shot, top5_zero_shot = compute_zero_shot_accuracy(model, test_loader, text_features_all, device)\n","\n","# Output zero-shot accuracy\n","print(f\"Zero-shot Top-1 Accuracy: {top1_zero_shot:.2f}%\")\n","print(f\"Zero-shot Top-3 Accuracy: {top3_zero_shot:.2f}%\")\n","print(f\"Zero-shot Top-5 Accuracy: {top5_zero_shot:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OeUevGOL6TyJ","executionInfo":{"status":"ok","timestamp":1744342728054,"user_tz":240,"elapsed":2126,"user":{"displayName":"Ava Collier","userId":"11682034821230281817"}},"outputId":"dd17782b-bcf6-4fb3-994a-0d74d4ed623b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Zero-shot Evaluation: 100%|██████████| 10/10 [00:02<00:00,  4.76it/s]"]},{"output_type":"stream","name":"stdout","text":["Zero-shot Top-1 Accuracy: 74.45%\n","Zero-shot Top-3 Accuracy: 91.17%\n","Zero-shot Top-5 Accuracy: 97.48%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}