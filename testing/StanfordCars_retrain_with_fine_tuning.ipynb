{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Retraining CLIP"],"metadata":{"id":"4KStJjS-IHyh"}},{"cell_type":"markdown","source":["## Fine Tuning Changes"],"metadata":{"id":"tj4MUaY2Fkwp"}},{"cell_type":"markdown","source":["1. Updated learning rate from 1e-4 to 1e-3.\n","\n","  This data had a lot of very similar/hard-to-distinguish classes. The previous learning rate wasn't able to capture the fine-tune differences quickly enough. Increasing the learning rate gave the MLP more powerful predictions. Overfitting was carefully monitored and did not present an issue.\n","\n","2. Changed the weighting of the logits from:\n","\n","  ```\n","  blended_logits = 0.5 * classifier_logits + 0.5 * clip_logits\n","  ```\n","  to:\n","  ```\n","  blended_logits = 0.9 * classifier_logits + 0.1 * clip_logits\n","  ```\n","\n","  The CLIP model was overfitting and getting bottle-necked in the similar phrasing across labels (i.e. Acura RL Sedan 2012 vs Acura TL Sedan 2012). Relying more heavily on the classifer allows noise-free classifier to do the bulk of the work, while still allowing the CLIP contrastive learning to guide the classifier slightly when it's unsure.  "],"metadata":{"id":"vvbhp0M5F-JF"}},{"cell_type":"markdown","source":["## Set Up"],"metadata":{"id":"9WzEZ_UCbzJ2"}},{"cell_type":"markdown","source":["Load in all our packages"],"metadata":{"id":"68YhHiFHbigh"}},{"cell_type":"code","source":["# Install necessary packages\n","!pip install ftfy regex tqdm\n","!pip install git+https://github.com/openai/CLIP.git\n","!pip install scipy\n","\n","import clip\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","from torchvision import transforms\n","from torchvision import datasets\n","from PIL import Image\n","import os\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","import shutil\n","import packaging\n","import kagglehub\n","import pandas as pd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lHMXkYSba1KH","outputId":"8fe0bb27-e69a-4810-9797-71d61dd0a4a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ftfy\n","  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n","Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: ftfy\n","Successfully installed ftfy-6.3.1\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-ljm_6v_d\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-ljm_6v_d\n","  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->clip==1.0)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->clip==1.0)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->clip==1.0)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->clip==1.0)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->clip==1.0)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->clip==1.0)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch->clip==1.0)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->clip==1.0)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->clip==1.0)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->clip==1.0)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m112.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=73c8f6ecbad32399e6bc0a6c5a097e63b416f8c6caddbab443994b62bbf71100\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-piv2hury/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n","Successfully built clip\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, clip\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed clip-1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.14.1)\n","Requirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (2.0.2)\n"]}]},{"cell_type":"code","source":["# Check PyTorch version\n","# Ensure compatibility with CUDA\n","version = packaging.version.parse(torch.__version__)\n","if version > packaging.version.parse('1.7.0'):\n","    print(\"Pytorch version is above 1.7.0\")\n","    print(\"It is version:\", version)\n","else:\n","    print(\"PyTorch version is not above 1.7.0. Please Upgrade\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Am9l2tDXa5vT","outputId":"2aab8303-48e9-40a4-f300-798d02fa9bbe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pytorch version is above 1.7.0\n","It is version: 2.6.0+cu124\n"]}]},{"cell_type":"markdown","source":["Get the Clip Model"],"metadata":{"id":"ramFc_fIbu4i"}},{"cell_type":"code","source":["# Load CLIP model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)\n","model = model.float()"],"metadata":{"id":"UPvdMxm4a8mr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"621bace9-1a03-4eff-d2b5-47f79762a40e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|███████████████████████████████████████| 338M/338M [00:24<00:00, 14.5MiB/s]\n"]}]},{"cell_type":"markdown","source":["### Unfreeze more layers from CLIP\n","\n","By default, many pre-trained models like CLIP freeze their internal layers. This means the weights of those layers don't get updated during training. Freezing maintains the extracted features from the initial training. But if we want the model to adapt to our new data, we need to \"unfreeze\" certain layers so they can be trained."],"metadata":{"id":"ZISC2-06WdLE"}},{"cell_type":"code","source":["for name, param in model.named_parameters():\n","    # This loop goes through every parameter (weight/bias) in the CLIP model.\n","    # `name` is a string describing which layer the parameter belongs to.\n","    # `param` is the actual parameter tensor (a PyTorch object containing weights).\n","\n","    if \"visual\" in name:\n","        # Only unfreeze layers in the \"visual\" part of the model.\n","        # CLIP has two main parts: a visual encoder (for images) and a text encoder (for text).\n","        # We only want to modify the visual encoder.\n","\n","        param.requires_grad = True\n","        # This tells PyTorch: \"Yes, this parameter should be updated during training.\"\n","        # Any parameter with `requires_grad = False` will be ignored during backpropagation.\n"],"metadata":{"id":"ol9ZYKNra_mc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###  Define linear classification head\n","\n","This is a very simple MLP neural network: a single fully connected linear layer. It's used to map the output of CLIP's image encoder to a set of class predictions. Think of it like the final decision layer that says: \"I think this image is class X.\""],"metadata":{"id":"tImqfz78XJsG"}},{"cell_type":"code","source":["class LinearClassifier(nn.Module):\n","    def __init__(self, input_dim, num_classes):\n","        # Constructor for the class. Called when we create an instance of LinearClassifier.\n","        # `input_dim` is the size of the input features (from the CLIP image encoder: 512).\n","        # `num_classes` is the number of categories we want to classify\n","\n","        super(LinearClassifier, self).__init__()\n","\n","        self.fc = nn.Linear(input_dim, num_classes)\n","        # This creates the linear (fully connected) layer.\n","        # It takes a vector of size `input_dim`\n","        # and outputs a vector of size `num_classes` with values\n","        # representing the similarity of an image to each class.\n","\n","    def forward(self, image_features):\n","        # This function defines how the data flows through the model during forward propogation.\n","        # It's called automatically during training and inference.\n","\n","        return self.fc(image_features)\n","        # The output is a set of raw scores (logits) for each class.\n"],"metadata":{"id":"hiaZm425bZFU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **DATA: This is the part you edit**"],"metadata":{"id":"MqGkpdBUqLKo"}},{"cell_type":"markdown","source":["To run this re-training procedure this is the **only** part you want to edit. All necessary changes can be made here. Changes elsewhere may effect the model and make them difficult to compare."],"metadata":{"id":"WH6UfxxJmFXz"}},{"cell_type":"markdown","source":["### Ok, now we actually do this on the cars data set"],"metadata":{"id":"hnUTNUhujS9c"}},{"cell_type":"markdown","source":["Load data"],"metadata":{"id":"sDqHKDmDxUJM"}},{"cell_type":"code","source":["# Download latest version\n","path = kagglehub.dataset_download(\"jutrera/stanford-car-dataset-by-classes-folder\")\n","print(\"Path to dataset files:\", path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zqOiUSclqNXA","outputId":"2334b653-1635-4527-f6fb-44819a7afd86"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading from https://www.kaggle.com/api/v1/datasets/download/jutrera/stanford-car-dataset-by-classes-folder?dataset_version_number=2...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1.83G/1.83G [01:23<00:00, 23.4MB/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting files...\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Path to dataset files: /root/.cache/kagglehub/datasets/jutrera/stanford-car-dataset-by-classes-folder/versions/2\n"]}]},{"cell_type":"markdown","source":["Split out validation set"],"metadata":{"id":"LvZbwKwlxVLR"}},{"cell_type":"code","source":["dataset_root = os.path.join(path, 'car_data', 'car_data', 'train')  # Path to your \"train\" folder, change as needed\n","val_root = \"/kaggle/working/cars/val\"  # Path to save the validation split (working directory), change as needed\n","# Define paths\n","\n","\n","if not os.path.exists(val_root):\n","    os.makedirs(val_root)\n","# Create the validation root folder if it doesn't exist\n","\n","\n","for class_name in os.listdir(dataset_root):\n","# Split data within each class folder\n","\n","    class_folder = os.path.join(dataset_root, class_name)\n","\n","    if os.path.isdir(class_folder):\n","\n","        image_files = [f for f in os.listdir(class_folder) if os.path.isfile(os.path.join(class_folder, f))]\n","        # Get list of image files in the class folder\n","\n","        train_files, val_files = train_test_split(image_files, test_size=0.2, random_state=42)\n","        # Split the images into training and validation sets\n","\n","        val_class_folder = os.path.join(val_root, class_name)\n","        if not os.path.exists(val_class_folder):\n","            os.makedirs(val_class_folder)\n","        # Create corresponding folders in the validation directory\n","\n","        for val_image in val_files:\n","            src = os.path.join(class_folder, val_image)\n","            dst = os.path.join(val_class_folder, val_image)\n","            shutil.copy(src, dst)  # Use copy instead of move\n","        # Copy validation images to the validation folder\n","\n","# After this, the validation set should be created in /kaggle/working/dogs/val\n"],"metadata":{"id":"21sGP7OGwU-M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Preprocess"],"metadata":{"id":"_O1QUnYtxZLR"}},{"cell_type":"code","source":["train_transform = preprocess\n","val_transform = preprocess\n","# Define the transformation for CLIP preprocessing (same as when we loaded the model)\n","# CLIP preprocess automatically resizes, normalizes, and converts to tensor\n","\n","train_dataset = datasets.ImageFolder(root=dataset_root, transform=train_transform)\n","val_dataset = datasets.ImageFolder(root=val_root, transform=val_transform)\n","# Create datasets for train and validation using ImageFolder\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","# Create DataLoaders for train and validation sets"],"metadata":{"id":"HOFSEO2hqXyo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Get classes"],"metadata":{"id":"xuTwmMQhxdoN"}},{"cell_type":"code","source":["class_names = train_dataset.classes\n","print(class_names)\n","# Extract class names from folders\n","\n","with torch.no_grad():\n","    all_text_prompts = [f\"A photo of a {classname}\" for classname in class_names]\n","    tokenized_texts = clip.tokenize(all_text_prompts).to(device)\n","    text_features_all = model.encode_text(tokenized_texts)\n","    text_features_all = F.normalize(text_features_all, dim=-1).float()  # <- add .float() here\n","# Update class names with text prompt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"My0rOfwSrP6h","outputId":"aa194e2b-cd13-47bc-a074-652843d16721"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['AM General Hummer SUV 2000', 'Acura Integra Type R 2001', 'Acura RL Sedan 2012', 'Acura TL Sedan 2012', 'Acura TL Type-S 2008', 'Acura TSX Sedan 2012', 'Acura ZDX Hatchback 2012', 'Aston Martin V8 Vantage Convertible 2012', 'Aston Martin V8 Vantage Coupe 2012', 'Aston Martin Virage Convertible 2012', 'Aston Martin Virage Coupe 2012', 'Audi 100 Sedan 1994', 'Audi 100 Wagon 1994', 'Audi A5 Coupe 2012', 'Audi R8 Coupe 2012', 'Audi RS 4 Convertible 2008', 'Audi S4 Sedan 2007', 'Audi S4 Sedan 2012', 'Audi S5 Convertible 2012', 'Audi S5 Coupe 2012', 'Audi S6 Sedan 2011', 'Audi TT Hatchback 2011', 'Audi TT RS Coupe 2012', 'Audi TTS Coupe 2012', 'Audi V8 Sedan 1994', 'BMW 1 Series Convertible 2012', 'BMW 1 Series Coupe 2012', 'BMW 3 Series Sedan 2012', 'BMW 3 Series Wagon 2012', 'BMW 6 Series Convertible 2007', 'BMW ActiveHybrid 5 Sedan 2012', 'BMW M3 Coupe 2012', 'BMW M5 Sedan 2010', 'BMW M6 Convertible 2010', 'BMW X3 SUV 2012', 'BMW X5 SUV 2007', 'BMW X6 SUV 2012', 'BMW Z4 Convertible 2012', 'Bentley Arnage Sedan 2009', 'Bentley Continental Flying Spur Sedan 2007', 'Bentley Continental GT Coupe 2007', 'Bentley Continental GT Coupe 2012', 'Bentley Continental Supersports Conv. Convertible 2012', 'Bentley Mulsanne Sedan 2011', 'Bugatti Veyron 16.4 Convertible 2009', 'Bugatti Veyron 16.4 Coupe 2009', 'Buick Enclave SUV 2012', 'Buick Rainier SUV 2007', 'Buick Regal GS 2012', 'Buick Verano Sedan 2012', 'Cadillac CTS-V Sedan 2012', 'Cadillac Escalade EXT Crew Cab 2007', 'Cadillac SRX SUV 2012', 'Chevrolet Avalanche Crew Cab 2012', 'Chevrolet Camaro Convertible 2012', 'Chevrolet Cobalt SS 2010', 'Chevrolet Corvette Convertible 2012', 'Chevrolet Corvette Ron Fellows Edition Z06 2007', 'Chevrolet Corvette ZR1 2012', 'Chevrolet Express Cargo Van 2007', 'Chevrolet Express Van 2007', 'Chevrolet HHR SS 2010', 'Chevrolet Impala Sedan 2007', 'Chevrolet Malibu Hybrid Sedan 2010', 'Chevrolet Malibu Sedan 2007', 'Chevrolet Monte Carlo Coupe 2007', 'Chevrolet Silverado 1500 Classic Extended Cab 2007', 'Chevrolet Silverado 1500 Extended Cab 2012', 'Chevrolet Silverado 1500 Hybrid Crew Cab 2012', 'Chevrolet Silverado 1500 Regular Cab 2012', 'Chevrolet Silverado 2500HD Regular Cab 2012', 'Chevrolet Sonic Sedan 2012', 'Chevrolet Tahoe Hybrid SUV 2012', 'Chevrolet TrailBlazer SS 2009', 'Chevrolet Traverse SUV 2012', 'Chrysler 300 SRT-8 2010', 'Chrysler Aspen SUV 2009', 'Chrysler Crossfire Convertible 2008', 'Chrysler PT Cruiser Convertible 2008', 'Chrysler Sebring Convertible 2010', 'Chrysler Town and Country Minivan 2012', 'Daewoo Nubira Wagon 2002', 'Dodge Caliber Wagon 2007', 'Dodge Caliber Wagon 2012', 'Dodge Caravan Minivan 1997', 'Dodge Challenger SRT8 2011', 'Dodge Charger SRT-8 2009', 'Dodge Charger Sedan 2012', 'Dodge Dakota Club Cab 2007', 'Dodge Dakota Crew Cab 2010', 'Dodge Durango SUV 2007', 'Dodge Durango SUV 2012', 'Dodge Journey SUV 2012', 'Dodge Magnum Wagon 2008', 'Dodge Ram Pickup 3500 Crew Cab 2010', 'Dodge Ram Pickup 3500 Quad Cab 2009', 'Dodge Sprinter Cargo Van 2009', 'Eagle Talon Hatchback 1998', 'FIAT 500 Abarth 2012', 'FIAT 500 Convertible 2012', 'Ferrari 458 Italia Convertible 2012', 'Ferrari 458 Italia Coupe 2012', 'Ferrari California Convertible 2012', 'Ferrari FF Coupe 2012', 'Fisker Karma Sedan 2012', 'Ford E-Series Wagon Van 2012', 'Ford Edge SUV 2012', 'Ford Expedition EL SUV 2009', 'Ford F-150 Regular Cab 2007', 'Ford F-150 Regular Cab 2012', 'Ford F-450 Super Duty Crew Cab 2012', 'Ford Fiesta Sedan 2012', 'Ford Focus Sedan 2007', 'Ford Freestar Minivan 2007', 'Ford GT Coupe 2006', 'Ford Mustang Convertible 2007', 'Ford Ranger SuperCab 2011', 'GMC Acadia SUV 2012', 'GMC Canyon Extended Cab 2012', 'GMC Savana Van 2012', 'GMC Terrain SUV 2012', 'GMC Yukon Hybrid SUV 2012', 'Geo Metro Convertible 1993', 'HUMMER H2 SUT Crew Cab 2009', 'HUMMER H3T Crew Cab 2010', 'Honda Accord Coupe 2012', 'Honda Accord Sedan 2012', 'Honda Odyssey Minivan 2007', 'Honda Odyssey Minivan 2012', 'Hyundai Accent Sedan 2012', 'Hyundai Azera Sedan 2012', 'Hyundai Elantra Sedan 2007', 'Hyundai Elantra Touring Hatchback 2012', 'Hyundai Genesis Sedan 2012', 'Hyundai Santa Fe SUV 2012', 'Hyundai Sonata Hybrid Sedan 2012', 'Hyundai Sonata Sedan 2012', 'Hyundai Tucson SUV 2012', 'Hyundai Veloster Hatchback 2012', 'Hyundai Veracruz SUV 2012', 'Infiniti G Coupe IPL 2012', 'Infiniti QX56 SUV 2011', 'Isuzu Ascender SUV 2008', 'Jaguar XK XKR 2012', 'Jeep Compass SUV 2012', 'Jeep Grand Cherokee SUV 2012', 'Jeep Liberty SUV 2012', 'Jeep Patriot SUV 2012', 'Jeep Wrangler SUV 2012', 'Lamborghini Aventador Coupe 2012', 'Lamborghini Diablo Coupe 2001', 'Lamborghini Gallardo LP 570-4 Superleggera 2012', 'Lamborghini Reventon Coupe 2008', 'Land Rover LR2 SUV 2012', 'Land Rover Range Rover SUV 2012', 'Lincoln Town Car Sedan 2011', 'MINI Cooper Roadster Convertible 2012', 'Maybach Landaulet Convertible 2012', 'Mazda Tribute SUV 2011', 'McLaren MP4-12C Coupe 2012', 'Mercedes-Benz 300-Class Convertible 1993', 'Mercedes-Benz C-Class Sedan 2012', 'Mercedes-Benz E-Class Sedan 2012', 'Mercedes-Benz S-Class Sedan 2012', 'Mercedes-Benz SL-Class Coupe 2009', 'Mercedes-Benz Sprinter Van 2012', 'Mitsubishi Lancer Sedan 2012', 'Nissan 240SX Coupe 1998', 'Nissan Juke Hatchback 2012', 'Nissan Leaf Hatchback 2012', 'Nissan NV Passenger Van 2012', 'Plymouth Neon Coupe 1999', 'Porsche Panamera Sedan 2012', 'Ram C-V Cargo Van Minivan 2012', 'Rolls-Royce Ghost Sedan 2012', 'Rolls-Royce Phantom Drophead Coupe Convertible 2012', 'Rolls-Royce Phantom Sedan 2012', 'Scion xD Hatchback 2012', 'Spyker C8 Convertible 2009', 'Spyker C8 Coupe 2009', 'Suzuki Aerio Sedan 2007', 'Suzuki Kizashi Sedan 2012', 'Suzuki SX4 Hatchback 2012', 'Suzuki SX4 Sedan 2012', 'Tesla Model S Sedan 2012', 'Toyota 4Runner SUV 2012', 'Toyota Camry Sedan 2012', 'Toyota Corolla Sedan 2012', 'Toyota Sequoia SUV 2012', 'Volkswagen Beetle Hatchback 2012', 'Volkswagen Golf Hatchback 1991', 'Volkswagen Golf Hatchback 2012', 'Volvo 240 Sedan 1993', 'Volvo C30 Hatchback 2012', 'Volvo XC90 SUV 2007', 'smart fortwo Convertible 2012']\n"]}]},{"cell_type":"markdown","source":["Get test set set up for later"],"metadata":{"id":"jPTip_n7xhZV"}},{"cell_type":"code","source":["# Paths\n","test_root = os.path.join(path, 'car_data', 'car_data', 'test') # Change this for your own data\n","\n","# Load test set\n","test_dataset = datasets.ImageFolder(root=test_root, transform=preprocess)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","# Get all class names\n","class_names_test = test_dataset.classes\n","print(\"Class names:\", class_names_test)\n","\n","# Generate text features for all classes once\n","with torch.no_grad():\n","    all_texts = [f\"A photo of a {classname}\" for classname in class_names_test] # Feel free to change the prompt is desired\n","    tokenized_texts = clip.tokenize(all_texts).to(device)\n","    text_features_all = model.encode_text(tokenized_texts)  # Shape: (num_classes, 512)"],"metadata":{"id":"jvfqV9oxnhsW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"72bba1ee-c50a-4fd8-b20c-6966d311f8d0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Class names: ['AM General Hummer SUV 2000', 'Acura Integra Type R 2001', 'Acura RL Sedan 2012', 'Acura TL Sedan 2012', 'Acura TL Type-S 2008', 'Acura TSX Sedan 2012', 'Acura ZDX Hatchback 2012', 'Aston Martin V8 Vantage Convertible 2012', 'Aston Martin V8 Vantage Coupe 2012', 'Aston Martin Virage Convertible 2012', 'Aston Martin Virage Coupe 2012', 'Audi 100 Sedan 1994', 'Audi 100 Wagon 1994', 'Audi A5 Coupe 2012', 'Audi R8 Coupe 2012', 'Audi RS 4 Convertible 2008', 'Audi S4 Sedan 2007', 'Audi S4 Sedan 2012', 'Audi S5 Convertible 2012', 'Audi S5 Coupe 2012', 'Audi S6 Sedan 2011', 'Audi TT Hatchback 2011', 'Audi TT RS Coupe 2012', 'Audi TTS Coupe 2012', 'Audi V8 Sedan 1994', 'BMW 1 Series Convertible 2012', 'BMW 1 Series Coupe 2012', 'BMW 3 Series Sedan 2012', 'BMW 3 Series Wagon 2012', 'BMW 6 Series Convertible 2007', 'BMW ActiveHybrid 5 Sedan 2012', 'BMW M3 Coupe 2012', 'BMW M5 Sedan 2010', 'BMW M6 Convertible 2010', 'BMW X3 SUV 2012', 'BMW X5 SUV 2007', 'BMW X6 SUV 2012', 'BMW Z4 Convertible 2012', 'Bentley Arnage Sedan 2009', 'Bentley Continental Flying Spur Sedan 2007', 'Bentley Continental GT Coupe 2007', 'Bentley Continental GT Coupe 2012', 'Bentley Continental Supersports Conv. Convertible 2012', 'Bentley Mulsanne Sedan 2011', 'Bugatti Veyron 16.4 Convertible 2009', 'Bugatti Veyron 16.4 Coupe 2009', 'Buick Enclave SUV 2012', 'Buick Rainier SUV 2007', 'Buick Regal GS 2012', 'Buick Verano Sedan 2012', 'Cadillac CTS-V Sedan 2012', 'Cadillac Escalade EXT Crew Cab 2007', 'Cadillac SRX SUV 2012', 'Chevrolet Avalanche Crew Cab 2012', 'Chevrolet Camaro Convertible 2012', 'Chevrolet Cobalt SS 2010', 'Chevrolet Corvette Convertible 2012', 'Chevrolet Corvette Ron Fellows Edition Z06 2007', 'Chevrolet Corvette ZR1 2012', 'Chevrolet Express Cargo Van 2007', 'Chevrolet Express Van 2007', 'Chevrolet HHR SS 2010', 'Chevrolet Impala Sedan 2007', 'Chevrolet Malibu Hybrid Sedan 2010', 'Chevrolet Malibu Sedan 2007', 'Chevrolet Monte Carlo Coupe 2007', 'Chevrolet Silverado 1500 Classic Extended Cab 2007', 'Chevrolet Silverado 1500 Extended Cab 2012', 'Chevrolet Silverado 1500 Hybrid Crew Cab 2012', 'Chevrolet Silverado 1500 Regular Cab 2012', 'Chevrolet Silverado 2500HD Regular Cab 2012', 'Chevrolet Sonic Sedan 2012', 'Chevrolet Tahoe Hybrid SUV 2012', 'Chevrolet TrailBlazer SS 2009', 'Chevrolet Traverse SUV 2012', 'Chrysler 300 SRT-8 2010', 'Chrysler Aspen SUV 2009', 'Chrysler Crossfire Convertible 2008', 'Chrysler PT Cruiser Convertible 2008', 'Chrysler Sebring Convertible 2010', 'Chrysler Town and Country Minivan 2012', 'Daewoo Nubira Wagon 2002', 'Dodge Caliber Wagon 2007', 'Dodge Caliber Wagon 2012', 'Dodge Caravan Minivan 1997', 'Dodge Challenger SRT8 2011', 'Dodge Charger SRT-8 2009', 'Dodge Charger Sedan 2012', 'Dodge Dakota Club Cab 2007', 'Dodge Dakota Crew Cab 2010', 'Dodge Durango SUV 2007', 'Dodge Durango SUV 2012', 'Dodge Journey SUV 2012', 'Dodge Magnum Wagon 2008', 'Dodge Ram Pickup 3500 Crew Cab 2010', 'Dodge Ram Pickup 3500 Quad Cab 2009', 'Dodge Sprinter Cargo Van 2009', 'Eagle Talon Hatchback 1998', 'FIAT 500 Abarth 2012', 'FIAT 500 Convertible 2012', 'Ferrari 458 Italia Convertible 2012', 'Ferrari 458 Italia Coupe 2012', 'Ferrari California Convertible 2012', 'Ferrari FF Coupe 2012', 'Fisker Karma Sedan 2012', 'Ford E-Series Wagon Van 2012', 'Ford Edge SUV 2012', 'Ford Expedition EL SUV 2009', 'Ford F-150 Regular Cab 2007', 'Ford F-150 Regular Cab 2012', 'Ford F-450 Super Duty Crew Cab 2012', 'Ford Fiesta Sedan 2012', 'Ford Focus Sedan 2007', 'Ford Freestar Minivan 2007', 'Ford GT Coupe 2006', 'Ford Mustang Convertible 2007', 'Ford Ranger SuperCab 2011', 'GMC Acadia SUV 2012', 'GMC Canyon Extended Cab 2012', 'GMC Savana Van 2012', 'GMC Terrain SUV 2012', 'GMC Yukon Hybrid SUV 2012', 'Geo Metro Convertible 1993', 'HUMMER H2 SUT Crew Cab 2009', 'HUMMER H3T Crew Cab 2010', 'Honda Accord Coupe 2012', 'Honda Accord Sedan 2012', 'Honda Odyssey Minivan 2007', 'Honda Odyssey Minivan 2012', 'Hyundai Accent Sedan 2012', 'Hyundai Azera Sedan 2012', 'Hyundai Elantra Sedan 2007', 'Hyundai Elantra Touring Hatchback 2012', 'Hyundai Genesis Sedan 2012', 'Hyundai Santa Fe SUV 2012', 'Hyundai Sonata Hybrid Sedan 2012', 'Hyundai Sonata Sedan 2012', 'Hyundai Tucson SUV 2012', 'Hyundai Veloster Hatchback 2012', 'Hyundai Veracruz SUV 2012', 'Infiniti G Coupe IPL 2012', 'Infiniti QX56 SUV 2011', 'Isuzu Ascender SUV 2008', 'Jaguar XK XKR 2012', 'Jeep Compass SUV 2012', 'Jeep Grand Cherokee SUV 2012', 'Jeep Liberty SUV 2012', 'Jeep Patriot SUV 2012', 'Jeep Wrangler SUV 2012', 'Lamborghini Aventador Coupe 2012', 'Lamborghini Diablo Coupe 2001', 'Lamborghini Gallardo LP 570-4 Superleggera 2012', 'Lamborghini Reventon Coupe 2008', 'Land Rover LR2 SUV 2012', 'Land Rover Range Rover SUV 2012', 'Lincoln Town Car Sedan 2011', 'MINI Cooper Roadster Convertible 2012', 'Maybach Landaulet Convertible 2012', 'Mazda Tribute SUV 2011', 'McLaren MP4-12C Coupe 2012', 'Mercedes-Benz 300-Class Convertible 1993', 'Mercedes-Benz C-Class Sedan 2012', 'Mercedes-Benz E-Class Sedan 2012', 'Mercedes-Benz S-Class Sedan 2012', 'Mercedes-Benz SL-Class Coupe 2009', 'Mercedes-Benz Sprinter Van 2012', 'Mitsubishi Lancer Sedan 2012', 'Nissan 240SX Coupe 1998', 'Nissan Juke Hatchback 2012', 'Nissan Leaf Hatchback 2012', 'Nissan NV Passenger Van 2012', 'Plymouth Neon Coupe 1999', 'Porsche Panamera Sedan 2012', 'Ram C-V Cargo Van Minivan 2012', 'Rolls-Royce Ghost Sedan 2012', 'Rolls-Royce Phantom Drophead Coupe Convertible 2012', 'Rolls-Royce Phantom Sedan 2012', 'Scion xD Hatchback 2012', 'Spyker C8 Convertible 2009', 'Spyker C8 Coupe 2009', 'Suzuki Aerio Sedan 2007', 'Suzuki Kizashi Sedan 2012', 'Suzuki SX4 Hatchback 2012', 'Suzuki SX4 Sedan 2012', 'Tesla Model S Sedan 2012', 'Toyota 4Runner SUV 2012', 'Toyota Camry Sedan 2012', 'Toyota Corolla Sedan 2012', 'Toyota Sequoia SUV 2012', 'Volkswagen Beetle Hatchback 2012', 'Volkswagen Golf Hatchback 1991', 'Volkswagen Golf Hatchback 2012', 'Volvo 240 Sedan 1993', 'Volvo C30 Hatchback 2012', 'Volvo XC90 SUV 2007', 'smart fortwo Convertible 2012']\n"]}]},{"cell_type":"markdown","source":["## **OK, STOP EDITING HERE**\n","\n","The rest of this file should work just fine without edits if you didn't change any variable names"],"metadata":{"id":"qjcXBTSjki5s"}},{"cell_type":"markdown","source":["## Let's Retrain"],"metadata":{"id":"jYoC_6-UYJ4V"}},{"cell_type":"code","source":["classifier = LinearClassifier(input_dim=512, num_classes=len(class_names)).to(device)\n","# Initializes the classifier we defined earlier\n","\n","optimizer = torch.optim.AdamW([\n","    {\"params\": model.visual.parameters(), \"lr\": 1e-6},\n","    {\"params\": classifier.parameters(), \"lr\": 1e-3}\n","], weight_decay=1e-4)\n","# We're training two parts:\n","# 1) model.visual: The vision encoder from CLIP — we fine-tune it very gently using a small learning rate (1e-6)\n","# 2) classifier: Our new linear layer — it starts from scratch, so we train it more aggressively (1e-4)\n","# AdamW is a common optimizer\n","\n","criterion = nn.CrossEntropyLoss()\n","# Cross-entropy compares the predicted scores (logits) against the true label and penalizes wrong guesses.\n","\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n","# This slowly reduces the learning rate over time in a smooth cosine curve\n","# this is a common trick to make training more stable and avoid overshooting the minimum loss.\n","\n","\n","num_epochs = 10\n","# how many times we loop through the whole dataset\n","\n","best_val_acc = 0\n","# keeps track of the best accuracy we've seen so far\n","\n","patience = 3\n","# For early stopping — we stop training if validation accuracy doesn’t improve for 3 straight epochs\n","# This trains more efficiently and prevents overfitting\n","\n","epochs_no_improve = 0\n","# how many times we've failed to beat our best accuracy\n","\n","for epoch in range(num_epochs):\n","\n","    classifier.train()\n","    # classifier.train() puts the model in training mode\n","\n","    total_loss, correct, total = 0, 0, 0\n","\n","    # Training ##################################################\n","\n","    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Train)\"):\n","\n","        images, labels = images.to(device), labels.to(device)\n","        image_features = model.encode_image(images).float()\n","        # Use CLIP’s vision model to encode the images into 512-dimension feature vectors\n","        image_features = F.normalize(image_features, dim=-1)\n","        # Normalize them (unit length) so comparisons (dot products) behave like cosine similarity\n","\n","        with torch.no_grad():\n","            clip_logits = image_features @ text_features_all.T  # (B, num_classes)\n","        # Dot product between image and text features. Gives similarity scores\n","        # (logits) between each image and all class names.\n","\n","        classifier_logits = classifier(image_features)\n","        # our classifier’s own guess — based on its trained weights\n","\n","        clip_logits = clip_logits / clip_logits.norm(dim=-1, keepdim=True)\n","        classifier_logits = classifier_logits / classifier_logits.norm(dim=-1, keepdim=True)\n","        # Normalize - ensures the same scale\n","\n","        blended_logits = 0.9 * classifier_logits + 0.1 * clip_logits\n","        # average the scores from CLIP and our linear classifier\n","\n","        loss = criterion(blended_logits, labels)\n","        # Calculate the loss from the blended prediction\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        # Clear old gradients, backpropagate new ones, and take an optimizer step\n","\n","        total_loss += loss.item()\n","        correct += (blended_logits.argmax(dim=1) == labels).sum().item()\n","        total += labels.size(0)\n","        # Count how many predictions were correct and update total loss and accuracy\n","\n","    train_acc = 100 * correct / total\n","    print(f\"Epoch {epoch+1}: Train Loss = {total_loss:.4f}, Train Acc = {train_acc:.2f}%\")\n","\n","    # Validation ################################################\n","\n","    classifier.eval()\n","    # Switch model to evaluation mode\n","\n","    correct, total = 0, 0\n","\n","    with torch.no_grad():\n","        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Val)\"):\n","            images, labels = images.to(device), labels.to(device)\n","            image_features = model.encode_image(images).float()\n","            image_features = F.normalize(image_features, dim=-1)\n","\n","            with torch.no_grad():\n","                clip_logits = image_features @ text_features_all.T\n","            classifier_logits = classifier(image_features)\n","            clip_logits = clip_logits / clip_logits.norm(dim=-1, keepdim=True)\n","            classifier_logits = classifier_logits / classifier_logits.norm(dim=-1, keepdim=True)\n","            blended_logits = 0.9 * classifier_logits + 0.1 * clip_logits\n","            correct += (blended_logits.argmax(dim=1) == labels).sum().item()\n","            total += labels.size(0)\n","\n","    val_acc = 100 * correct / total\n","    print(f\"Epoch {epoch+1}: Val Acc = {val_acc:.2f}%\")\n","    # Count correct predictions to compute validation accuracy\n","\n","    # Early Stopping #############################################\n","\n","    if val_acc > best_val_acc:\n","        best_val_acc = val_acc\n","        epochs_no_improve = 0\n","        torch.save(classifier.state_dict(), 'best_linear_classifier.pth')\n","        print(\"Improved validation accuracy. Saved model.\")\n","    # If we beat our best validation accuracy, save the model\n","\n","    else:\n","        epochs_no_improve += 1\n","        if epochs_no_improve >= patience:\n","            print(\"Early stopping.\")\n","            break\n","    # If we’ve gone patience epochs with no improvement, stop training early\n","\n","    scheduler.step()\n","    # Move along the cosine schedule — lower the learning rate a bit\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6O2clmHwvIkl","outputId":"30f93faa-853f-4fe4-bc2c-58181dbf1447"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1/10 (Train): 100%|██████████| 255/255 [01:59<00:00,  2.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Train Loss = 1287.9661, Train Acc = 39.91%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/10 (Val): 100%|██████████| 54/54 [00:20<00:00,  2.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Val Acc = 71.74%\n","Improved validation accuracy. Saved model.\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/10 (Train): 100%|██████████| 255/255 [02:00<00:00,  2.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Train Loss = 1245.6002, Train Acc = 73.00%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/10 (Val): 100%|██████████| 54/54 [00:20<00:00,  2.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Val Acc = 85.72%\n","Improved validation accuracy. Saved model.\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/10 (Train): 100%|██████████| 255/255 [02:00<00:00,  2.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3: Train Loss = 1226.3566, Train Acc = 82.34%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/10 (Val): 100%|██████████| 54/54 [00:20<00:00,  2.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3: Val Acc = 89.48%\n","Improved validation accuracy. Saved model.\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/10 (Train): 100%|██████████| 255/255 [02:00<00:00,  2.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4: Train Loss = 1212.0805, Train Acc = 88.08%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/10 (Val): 100%|██████████| 54/54 [00:20<00:00,  2.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4: Val Acc = 91.07%\n","Improved validation accuracy. Saved model.\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/10 (Train): 100%|██████████| 255/255 [02:00<00:00,  2.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5: Train Loss = 1201.2098, Train Acc = 91.43%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/10 (Val): 100%|██████████| 54/54 [00:20<00:00,  2.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5: Val Acc = 94.65%\n","Improved validation accuracy. Saved model.\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/10 (Train): 100%|██████████| 255/255 [01:59<00:00,  2.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6: Train Loss = 1192.9454, Train Acc = 93.55%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/10 (Val): 100%|██████████| 54/54 [00:20<00:00,  2.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6: Val Acc = 95.06%\n","Improved validation accuracy. Saved model.\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/10 (Train): 100%|██████████| 255/255 [02:00<00:00,  2.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7: Train Loss = 1186.8657, Train Acc = 94.92%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/10 (Val): 100%|██████████| 54/54 [00:20<00:00,  2.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7: Val Acc = 96.18%\n","Improved validation accuracy. Saved model.\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/10 (Train): 100%|██████████| 255/255 [01:59<00:00,  2.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8: Train Loss = 1182.5136, Train Acc = 96.11%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/10 (Val): 100%|██████████| 54/54 [00:20<00:00,  2.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8: Val Acc = 96.18%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/10 (Train): 100%|██████████| 255/255 [02:00<00:00,  2.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9: Train Loss = 1179.8312, Train Acc = 96.43%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/10 (Val): 100%|██████████| 54/54 [00:20<00:00,  2.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9: Val Acc = 96.65%\n","Improved validation accuracy. Saved model.\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/10 (Train): 100%|██████████| 255/255 [01:59<00:00,  2.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10: Train Loss = 1178.3890, Train Acc = 96.70%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/10 (Val): 100%|██████████| 54/54 [00:20<00:00,  2.65it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 10: Val Acc = 96.71%\n","Improved validation accuracy. Saved model.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["## Compute Accuracy with Newly Trained Model"],"metadata":{"id":"wDH6f6RZ1HGc"}},{"cell_type":"code","source":["def compute_topk_accuracy(logits, labels, topk=(1, 3, 5)):\n","    max_k = max(topk)\n","    batch_size = labels.size(0)\n","\n","    _, pred = logits.topk(max_k, dim=1, largest=True, sorted=True)\n","    pred = pred.t()\n","    correct = pred.eq(labels.view(1, -1).expand_as(pred))\n","\n","    topk_accs = {}\n","    for k in topk:\n","        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n","        topk_accs[f\"top{k}\"] = (correct_k / batch_size).item() * 100.0\n","\n","    return topk_accs\n","\n","# Evaluate fine-tuned classifier\n","classifier.eval()\n","top1_total, top3_total, top5_total, total_samples = 0, 0, 0, 0\n","\n","with torch.no_grad():\n","    for images, labels in tqdm(test_loader, desc=\"Evaluating Fine-tuned Classifier\"):\n","        images, labels = images.to(device), labels.to(device)\n","        image_features = model.encode_image(images).float()\n","        image_features = F.normalize(image_features, dim=-1)\n","\n","        logits = classifier(image_features)\n","        accs = compute_topk_accuracy(logits, labels)\n","\n","        top1_total += accs['top1'] * images.size(0)\n","        top3_total += accs['top3'] * images.size(0)\n","        top5_total += accs['top5'] * images.size(0)\n","        total_samples += images.size(0)\n","\n","print(f\"\\nFine-tuned Classifier Accuracy:\")\n","print(f\"Top-1: {top1_total / total_samples:.2f}%\")\n","print(f\"Top-3: {top3_total / total_samples:.2f}%\")\n","print(f\"Top-5: {top5_total / total_samples:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PqmSJM0h1bqP","outputId":"17789249-a8ac-488e-fecd-7c66bebb50bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Evaluating Fine-tuned Classifier: 100%|██████████| 252/252 [01:30<00:00,  2.77it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Fine-tuned Classifier Accuracy:\n","Top-1: 81.05%\n","Top-3: 94.63%\n","Top-5: 96.80%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["## Compare To Zero Shot Accuracy"],"metadata":{"id":"owv0DDDIxp5s"}},{"cell_type":"code","source":["original_model = clip.load(\"ViT-B/32\", device=device)[0].float().eval()\n","with torch.no_grad():\n","    tokenized_texts = clip.tokenize([f\"A photo of a {classname}\" for classname in class_names_test]).to(device)\n","    text_features_all = original_model.encode_text(tokenized_texts)\n","    text_features_all = F.normalize(text_features_all, dim=-1).float()\n","\n","def compute_zero_shot_topk_accuracy(model, image_loader, text_features_all, device):\n","    model.eval()\n","    text_features_all = F.normalize(text_features_all, dim=-1)\n","\n","    top1_total, top3_total, top5_total, total_samples = 0, 0, 0, 0\n","\n","    with torch.no_grad():\n","        for images, labels in tqdm(image_loader, desc=\"Evaluating Zero-Shot CLIP\"):\n","            images, labels = images.to(device), labels.to(device)\n","            image_features = model.encode_image(images).float()\n","            image_features = F.normalize(image_features, dim=-1)\n","\n","            logits = image_features @ text_features_all.T\n","            accs = compute_topk_accuracy(logits, labels)\n","\n","            top1_total += accs['top1'] * images.size(0)\n","            top3_total += accs['top3'] * images.size(0)\n","            top5_total += accs['top5'] * images.size(0)\n","            total_samples += images.size(0)\n","\n","    return {\n","        'top1': top1_total / total_samples,\n","        'top3': top3_total / total_samples,\n","        'top5': top5_total / total_samples,\n","    }\n","\n","# Run zero-shot evaluation\n","zero_shot_results = compute_zero_shot_topk_accuracy(original_model, test_loader, text_features_all, device)\n","\n","print(\"\\nZero-Shot CLIP Accuracy:\")\n","print(f\"Top-1: {zero_shot_results['top1']:.2f}%\")\n","print(f\"Top-3: {zero_shot_results['top3']:.2f}%\")\n","print(f\"Top-5: {zero_shot_results['top5']:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SG_A74iYEQFl","outputId":"62a5fedc-ca59-4ea8-8324-2e018ac9cf20"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Evaluating Zero-Shot CLIP: 100%|██████████| 252/252 [01:30<00:00,  2.78it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Zero-Shot CLIP Accuracy:\n","Top-1: 58.90%\n","Top-3: 82.05%\n","Top-5: 90.03%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}