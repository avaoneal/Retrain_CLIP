{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Retraining CLIP"],"metadata":{"id":"4KStJjS-IHyh"}},{"cell_type":"markdown","source":["## Set Up"],"metadata":{"id":"9WzEZ_UCbzJ2"}},{"cell_type":"markdown","source":["Load in all our packages"],"metadata":{"id":"68YhHiFHbigh"}},{"cell_type":"code","source":["# Install necessary packages\n","!pip install ftfy regex tqdm\n","!pip install git+https://github.com/openai/CLIP.git\n","!pip install scipy\n","\n","import clip\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","from torchvision import transforms\n","from torchvision import datasets\n","from PIL import Image\n","import os\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","import shutil\n","import packaging\n","import kagglehub\n","import pandas as pd\n","import json"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lHMXkYSba1KH","outputId":"a31f1070-56ef-4cac-8a4e-c444aa4b0a42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ftfy\n","  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n","Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: ftfy\n","Successfully installed ftfy-6.3.1\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-w7vjnjwo\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-w7vjnjwo\n","  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->clip==1.0)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->clip==1.0)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->clip==1.0)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->clip==1.0)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->clip==1.0)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->clip==1.0)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch->clip==1.0)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->clip==1.0)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->clip==1.0)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->clip==1.0)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=b37125193a1ea16e34f03d2b5c9486b2511da334c4c892bc8ed49969cda9c0c4\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-2vm04b07/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n","Successfully built clip\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, clip\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed clip-1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.14.1)\n","Requirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (2.0.2)\n"]}]},{"cell_type":"code","source":["# Check PyTorch version\n","# Ensure compatibility with CUDA\n","version = packaging.version.parse(torch.__version__)\n","if version > packaging.version.parse('1.7.0'):\n","    print(\"Pytorch version is above 1.7.0\")\n","    print(\"It is version:\", version)\n","else:\n","    print(\"PyTorch version is not above 1.7.0. Please Upgrade\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Am9l2tDXa5vT","outputId":"e70dd7a4-00f1-4afc-da18-039368d5eae2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pytorch version is above 1.7.0\n","It is version: 2.6.0+cu124\n"]}]},{"cell_type":"markdown","source":["Get the Clip Model"],"metadata":{"id":"ramFc_fIbu4i"}},{"cell_type":"code","source":["# Load CLIP model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)\n","model = model.float()"],"metadata":{"id":"UPvdMxm4a8mr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fb5681e4-876a-4dae-c352-2f8599f19f08"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|███████████████████████████████████████| 338M/338M [00:12<00:00, 29.0MiB/s]\n"]}]},{"cell_type":"markdown","source":["### Unfreeze more layers from CLIP\n","\n","By default, many pre-trained models like CLIP freeze their internal layers. This means the weights of those layers don't get updated during training. Freezing maintains the extracted features from the initial training. But if we want the model to adapt to our new data, we need to \"unfreeze\" certain layers so they can be trained."],"metadata":{"id":"ZISC2-06WdLE"}},{"cell_type":"code","source":["for name, param in model.named_parameters():\n","    # This loop goes through every parameter (weight/bias) in the CLIP model.\n","    # `name` is a string describing which layer the parameter belongs to.\n","    # `param` is the actual parameter tensor (a PyTorch object containing weights).\n","\n","    if \"visual\" in name:\n","        # Only unfreeze layers in the \"visual\" part of the model.\n","        # CLIP has two main parts: a visual encoder (for images) and a text encoder (for text).\n","        # We only want to modify the visual encoder.\n","\n","        param.requires_grad = True\n","        # This tells PyTorch: \"Yes, this parameter should be updated during training.\"\n","        # Any parameter with `requires_grad = False` will be ignored during backpropagation.\n"],"metadata":{"id":"ol9ZYKNra_mc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###  Define linear classification head\n","\n","This is a very simple MLP neural network: a single fully connected linear layer. It's used to map the output of CLIP's image encoder to a set of class predictions. Think of it like the final decision layer that says: \"I think this image is class X.\""],"metadata":{"id":"tImqfz78XJsG"}},{"cell_type":"code","source":["class LinearClassifier(nn.Module):\n","    def __init__(self, input_dim, num_classes):\n","        # Constructor for the class. Called when we create an instance of LinearClassifier.\n","        # `input_dim` is the size of the input features (from the CLIP image encoder: 512).\n","        # `num_classes` is the number of categories we want to classify\n","\n","        super(LinearClassifier, self).__init__()\n","\n","        self.fc = nn.Linear(input_dim, num_classes)\n","        # This creates the linear (fully connected) layer.\n","        # It takes a vector of size `input_dim`\n","        # and outputs a vector of size `num_classes` with values\n","        # representing the similarity of an image to each class.\n","\n","    def forward(self, image_features):\n","        # This function defines how the data flows through the model during forward propogation.\n","        # It's called automatically during training and inference.\n","\n","        return self.fc(image_features)\n","        # The output is a set of raw scores (logits) for each class.\n"],"metadata":{"id":"hiaZm425bZFU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **DATA: This is the part you edit**"],"metadata":{"id":"MqGkpdBUqLKo"}},{"cell_type":"markdown","source":["To run this re-training procedure this is the **only** part you want to edit. All necessary changes can be made here. Changes elsewhere may effect the model and make them difficult to compare."],"metadata":{"id":"WH6UfxxJmFXz"}},{"cell_type":"markdown","source":["### Ok, now we actually do this on flowers data set"],"metadata":{"id":"hnUTNUhujS9c"}},{"cell_type":"markdown","source":["Load data"],"metadata":{"id":"sDqHKDmDxUJM"}},{"cell_type":"code","source":["# Download latest version\n","path = kagglehub.dataset_download(\"yousefmohamed20/oxford-102-flower-dataset\")\n","path = os.path.join(path, '102 flower')\n","print(\"Path to dataset files:\", path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zqOiUSclqNXA","outputId":"6a4b17f7-89fe-465f-db96-5ab51ad6ec6e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading from https://www.kaggle.com/api/v1/datasets/download/yousefmohamed20/oxford-102-flower-dataset?dataset_version_number=1...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 331M/331M [00:16<00:00, 21.5MB/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting files...\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Path to dataset files: /root/.cache/kagglehub/datasets/yousefmohamed20/oxford-102-flower-dataset/versions/1/102 flower\n"]}]},{"cell_type":"markdown","source":["Split out validation set"],"metadata":{"id":"LvZbwKwlxVLR"}},{"cell_type":"code","source":["dataset_root = os.path.join(path, 'flowers', 'train')\n","val_root = os.path.join(path, 'flowers', 'valid')\n","test_root = os.path.join(path, 'flowers', 'test')"],"metadata":{"id":"21sGP7OGwU-M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Preprocess and Get Classes"],"metadata":{"id":"7y67jHcB2XeI"}},{"cell_type":"code","source":["train_dataset = datasets.ImageFolder(root=dataset_root, transform=preprocess)\n","val_dataset = datasets.ImageFolder(root=val_root, transform=preprocess)\n","test_dataset = datasets.ImageFolder(root=test_root, transform=preprocess)\n","\n","with open(os.path.join(path, \"cat_to_name.json\"), \"r\") as f:\n","    idx_to_class = json.load(f)\n","\n","# This gives you the folder names in the exact order used for label indices\n","class_folders = train_dataset.classes  # e.g., ['1', '2', ..., '102']\n","class_names = [idx_to_class[folder] for folder in class_folders]\n","print(\"Class names:\", class_names)\n","# Extract class names from folders\n","\n","with torch.no_grad():\n","    all_text_prompts = [f\"A photo of a {classname}\" for classname in class_names]\n","    tokenized_texts = clip.tokenize(all_text_prompts).to(device)\n","    text_features_all = model.encode_text(tokenized_texts)\n","    text_features_all = F.normalize(text_features_all, dim=-1).float()"],"metadata":{"id":"0-gjJuZG2Xrs","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0ebd5bff-ba87-45bd-fdae-ece1b342bad7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Class names: ['pink primrose', 'globe thistle', 'blanket flower', 'trumpet creeper', 'blackberry lily', 'snapdragon', \"colt's foot\", 'king protea', 'spear thistle', 'yellow iris', 'globe-flower', 'purple coneflower', 'peruvian lily', 'balloon flower', 'hard-leaved pocket orchid', 'giant white arum lily', 'fire lily', 'pincushion flower', 'fritillary', 'red ginger', 'grape hyacinth', 'corn poppy', 'prince of wales feathers', 'stemless gentian', 'artichoke', 'canterbury bells', 'sweet william', 'carnation', 'garden phlox', 'love in the mist', 'mexican aster', 'alpine sea holly', 'ruby-lipped cattleya', 'cape flower', 'great masterwort', 'siam tulip', 'sweet pea', 'lenten rose', 'barbeton daisy', 'daffodil', 'sword lily', 'poinsettia', 'bolero deep blue', 'wallflower', 'marigold', 'buttercup', 'oxeye daisy', 'english marigold', 'common dandelion', 'petunia', 'wild pansy', 'primula', 'sunflower', 'pelargonium', 'bishop of llandaff', 'gaura', 'geranium', 'orange dahlia', 'tiger lily', 'pink-yellow dahlia', 'cautleya spicata', 'japanese anemone', 'black-eyed susan', 'silverbush', 'californian poppy', 'osteospermum', 'spring crocus', 'bearded iris', 'windflower', 'moon orchid', 'tree poppy', 'gazania', 'azalea', 'water lily', 'rose', 'thorn apple', 'morning glory', 'passion flower', 'lotus lotus', 'toad lily', 'bird of paradise', 'anthurium', 'frangipani', 'clematis', 'hibiscus', 'columbine', 'desert-rose', 'tree mallow', 'magnolia', 'cyclamen', 'watercress', 'monkshood', 'canna lily', 'hippeastrum', 'bee balm', 'ball moss', 'foxglove', 'bougainvillea', 'camellia', 'mallow', 'mexican petunia', 'bromelia']\n"]}]},{"cell_type":"code","source":["def fix_dataset_class_names(dataset, idx_to_class):\n","    # Fix class_to_idx: folder name -> index\n","    dataset.class_to_idx = {k: int(k) for k in idx_to_class.keys()}\n","\n","    # Fix classes: list of human-readable names in the same order as folder names\n","    dataset.classes = [idx_to_class[k] for k in sorted(dataset.class_to_idx.keys())]\n","\n","    return dataset"],"metadata":{"id":"jdI4BrsF5f0H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = fix_dataset_class_names(train_dataset, idx_to_class)\n","val_dataset = fix_dataset_class_names(val_dataset, idx_to_class)\n","test_dataset = fix_dataset_class_names(test_dataset, idx_to_class)\n","# Connect numeric labels to text labels\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"],"metadata":{"id":"HOFSEO2hqXyo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Get test set set up for later"],"metadata":{"id":"jPTip_n7xhZV"}},{"cell_type":"code","source":["test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n","class_names_test = test_dataset.classes  # Already the correct, human-readable names\n","\n","with torch.no_grad():\n","    all_text_prompts = [f\"A photo of a {classname}\" for classname in class_names_test]\n","    tokenized_texts = clip.tokenize(all_text_prompts).to(device)\n","    text_features_all = model.encode_text(tokenized_texts)\n","    text_features_all = F.normalize(text_features_all, dim=-1).float()"],"metadata":{"id":"jvfqV9oxnhsW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **OK, STOP EDITING HERE**\n","\n","The rest of this file should work just fine without edits if you didn't change any variable names"],"metadata":{"id":"qjcXBTSjki5s"}},{"cell_type":"markdown","source":["## Let's Retrain"],"metadata":{"id":"jYoC_6-UYJ4V"}},{"cell_type":"code","source":["classifier = LinearClassifier(input_dim=512, num_classes=len(class_names)).to(device)\n","# Initializes the classifier we defined earlier\n","\n","optimizer = torch.optim.AdamW([\n","    {\"params\": model.visual.parameters(), \"lr\": 1e-6},\n","    {\"params\": classifier.parameters(), \"lr\": 1e-4}\n","], weight_decay=1e-4)\n","# We're training two parts:\n","# 1) model.visual: The vision encoder from CLIP — we fine-tune it very gently using a small learning rate (1e-6)\n","# 2) classifier: Our new linear layer — it starts from scratch, so we train it more aggressively (1e-4)\n","# AdamW is a common optimizer\n","\n","criterion = nn.CrossEntropyLoss()\n","# Cross-entropy compares the predicted scores (logits) against the true label and penalizes wrong guesses.\n","\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n","# This slowly reduces the learning rate over time in a smooth cosine curve\n","# this is a common trick to make training more stable and avoid overshooting the minimum loss.\n","\n","\n","num_epochs = 10\n","# how many times we loop through the whole dataset\n","\n","best_val_acc = 0\n","# keeps track of the best accuracy we've seen so far\n","\n","patience = 3\n","# For early stopping — we stop training if validation accuracy doesn’t improve for 3 straight epochs\n","# This trains more efficiently and prevents overfitting\n","\n","epochs_no_improve = 0\n","# how many times we've failed to beat our best accuracy\n","\n","for epoch in range(num_epochs):\n","\n","    classifier.train()\n","    # classifier.train() puts the model in training mode\n","\n","    total_loss, correct, total = 0, 0, 0\n","\n","    # Training ##################################################\n","\n","    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Train)\"):\n","\n","        images, labels = images.to(device), labels.to(device)\n","        image_features = model.encode_image(images).float()\n","        # Use CLIP’s vision model to encode the images into 512-dimension feature vectors\n","        image_features = F.normalize(image_features, dim=-1)\n","        # Normalize them (unit length) so comparisons (dot products) behave like cosine similarity\n","\n","        with torch.no_grad():\n","            clip_logits = image_features @ text_features_all.T  # (B, num_classes)\n","        # Dot product between image and text features. Gives similarity scores\n","        # (logits) between each image and all class names.\n","\n","        classifier_logits = classifier(image_features)\n","        # our classifier’s own guess — based on its trained weights\n","\n","        clip_logits = clip_logits / clip_logits.norm(dim=-1, keepdim=True)\n","        classifier_logits = classifier_logits / classifier_logits.norm(dim=-1, keepdim=True)\n","        # Normalize - ensures the same scale\n","\n","        blended_logits = 0.5 * classifier_logits + 0.5 * clip_logits\n","        # average the scores from CLIP and our linear classifier\n","\n","        loss = criterion(blended_logits, labels)\n","        # Calculate the loss from the blended prediction\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        # Clear old gradients, backpropagate new ones, and take an optimizer step\n","\n","        total_loss += loss.item()\n","        correct += (blended_logits.argmax(dim=1) == labels).sum().item()\n","        total += labels.size(0)\n","        # Count how many predictions were correct and update total loss and accuracy\n","\n","    train_acc = 100 * correct / total\n","    print(f\"Epoch {epoch+1}: Train Loss = {total_loss:.4f}, Train Acc = {train_acc:.2f}%\")\n","\n","    # Validation ################################################\n","\n","    classifier.eval()\n","    # Switch model to evaluation mode\n","\n","    correct, total = 0, 0\n","\n","    with torch.no_grad():\n","        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Val)\"):\n","            images, labels = images.to(device), labels.to(device)\n","            image_features = model.encode_image(images).float()\n","            image_features = F.normalize(image_features, dim=-1)\n","\n","            with torch.no_grad():\n","                clip_logits = image_features @ text_features_all.T\n","            classifier_logits = classifier(image_features)\n","            clip_logits = clip_logits / clip_logits.norm(dim=-1, keepdim=True)\n","            classifier_logits = classifier_logits / classifier_logits.norm(dim=-1, keepdim=True)\n","            logits = 0.5 * classifier_logits + 0.5 * clip_logits\n","            correct += (logits.argmax(dim=1) == labels).sum().item()\n","            total += labels.size(0)\n","\n","    val_acc = 100 * correct / total\n","    print(f\"Epoch {epoch+1}: Val Acc = {val_acc:.2f}%\")\n","    # Count correct predictions to compute validation accuracy\n","\n","    # Early Stopping #############################################\n","\n","    if val_acc > best_val_acc:\n","        best_val_acc = val_acc\n","        epochs_no_improve = 0\n","        torch.save(classifier.state_dict(), 'best_linear_classifier.pth')\n","        print(\"Improved validation accuracy. Saved model.\")\n","    # If we beat our best validation accuracy, save the model\n","\n","    else:\n","        epochs_no_improve += 1\n","        if epochs_no_improve >= patience:\n","            print(\"Early stopping.\")\n","            break\n","    # If we’ve gone patience epochs with no improvement, stop training early\n","\n","    scheduler.step()\n","    # Move along the cosine schedule — lower the learning rate a bit\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6O2clmHwvIkl","outputId":"a7997a9f-4897-4f53-d089-57b2662d31d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1/10 (Train): 100%|██████████| 205/205 [01:03<00:00,  3.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Train Loss = 918.3882, Train Acc = 42.34%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/10 (Val): 100%|██████████| 26/26 [00:06<00:00,  3.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Val Acc = 78.24%\n","Improved validation accuracy. Saved model.\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/10 (Train): 100%|██████████| 205/205 [01:03<00:00,  3.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Train Loss = 885.5652, Train Acc = 88.00%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/10 (Val): 100%|██████████| 26/26 [00:06<00:00,  3.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Val Acc = 90.46%\n","Improved validation accuracy. Saved model.\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/10 (Train): 100%|██████████| 205/205 [01:03<00:00,  3.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3: Train Loss = 872.8288, Train Acc = 95.31%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/10 (Val): 100%|██████████| 26/26 [00:06<00:00,  4.01it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3: Val Acc = 94.99%\n","Improved validation accuracy. Saved model.\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/10 (Train): 100%|██████████| 205/205 [01:03<00:00,  3.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4: Train Loss = 865.0227, Train Acc = 97.83%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/10 (Val): 100%|██████████| 26/26 [00:06<00:00,  4.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4: Val Acc = 96.21%\n","Improved validation accuracy. Saved model.\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/10 (Train): 100%|██████████| 205/205 [01:03<00:00,  3.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5: Train Loss = 859.9346, Train Acc = 98.79%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/10 (Val): 100%|██████████| 26/26 [00:06<00:00,  4.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5: Val Acc = 96.58%\n","Improved validation accuracy. Saved model.\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/10 (Train): 100%|██████████| 205/205 [01:03<00:00,  3.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6: Train Loss = 856.5369, Train Acc = 99.15%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/10 (Val): 100%|██████████| 26/26 [00:06<00:00,  4.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6: Val Acc = 97.43%\n","Improved validation accuracy. Saved model.\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/10 (Train): 100%|██████████| 205/205 [01:03<00:00,  3.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7: Train Loss = 854.3147, Train Acc = 99.40%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/10 (Val): 100%|██████████| 26/26 [00:06<00:00,  4.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7: Val Acc = 97.43%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/10 (Train): 100%|██████████| 205/205 [01:03<00:00,  3.24it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8: Train Loss = 852.9565, Train Acc = 99.66%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/10 (Val): 100%|██████████| 26/26 [00:06<00:00,  4.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8: Val Acc = 97.56%\n","Improved validation accuracy. Saved model.\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/10 (Train): 100%|██████████| 205/205 [01:03<00:00,  3.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9: Train Loss = 852.1830, Train Acc = 99.73%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/10 (Val): 100%|██████████| 26/26 [00:06<00:00,  3.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9: Val Acc = 97.43%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/10 (Train): 100%|██████████| 205/205 [01:03<00:00,  3.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10: Train Loss = 851.8389, Train Acc = 99.74%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/10 (Val): 100%|██████████| 26/26 [00:06<00:00,  3.98it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 10: Val Acc = 97.56%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["## Compute Accuracy with Newly Trained Model"],"metadata":{"id":"wDH6f6RZ1HGc"}},{"cell_type":"code","source":["def compute_topk_accuracy(logits, labels, topk=(1, 3, 5)):\n","    max_k = max(topk)\n","    batch_size = labels.size(0)\n","\n","    _, pred = logits.topk(max_k, dim=1, largest=True, sorted=True)\n","    pred = pred.t()\n","    correct = pred.eq(labels.view(1, -1).expand_as(pred))\n","\n","    topk_accs = {}\n","    for k in topk:\n","        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n","        topk_accs[f\"top{k}\"] = (correct_k / batch_size).item() * 100.0\n","\n","    return topk_accs\n","\n","# Evaluate fine-tuned classifier\n","classifier.eval()\n","top1_total, top3_total, top5_total, total_samples = 0, 0, 0, 0\n","\n","with torch.no_grad():\n","    for images, labels in tqdm(test_loader, desc=\"Evaluating Fine-tuned Classifier\"):\n","        images, labels = images.to(device), labels.to(device)\n","        image_features = model.encode_image(images).float()\n","        image_features = F.normalize(image_features, dim=-1)\n","\n","        logits = classifier(image_features)\n","        accs = compute_topk_accuracy(logits, labels)\n","\n","        top1_total += accs['top1'] * images.size(0)\n","        top3_total += accs['top3'] * images.size(0)\n","        top5_total += accs['top5'] * images.size(0)\n","        total_samples += images.size(0)\n","\n","print(f\"\\nFine-tuned Classifier Accuracy:\")\n","print(f\"Top-1: {top1_total / total_samples:.2f}%\")\n","print(f\"Top-3: {top3_total / total_samples:.2f}%\")\n","print(f\"Top-5: {top5_total / total_samples:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PqmSJM0h1bqP","outputId":"b358c131-8118-4fe0-de2a-261a60faa2ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Evaluating Fine-tuned Classifier: 100%|██████████| 26/26 [00:06<00:00,  4.01it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Fine-tuned Classifier Accuracy:\n","Top-1: 96.09%\n","Top-3: 99.39%\n","Top-5: 99.63%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["## Compare To Zero Shot Accuracy"],"metadata":{"id":"owv0DDDIxp5s"}},{"cell_type":"code","source":["original_model = clip.load(\"ViT-B/32\", device=device)[0].float().eval()\n","with torch.no_grad():\n","    tokenized_texts = clip.tokenize([f\"A photo of a {classname}\" for classname in class_names_test]).to(device)\n","    text_features_all = original_model.encode_text(tokenized_texts)\n","    text_features_all = F.normalize(text_features_all, dim=-1).float()\n","\n","def compute_zero_shot_topk_accuracy(model, image_loader, text_features_all, device):\n","    model.eval()\n","    text_features_all = F.normalize(text_features_all, dim=-1)\n","\n","    top1_total, top3_total, top5_total, total_samples = 0, 0, 0, 0\n","\n","    with torch.no_grad():\n","        for images, labels in tqdm(image_loader, desc=\"Evaluating Zero-Shot CLIP\"):\n","            images, labels = images.to(device), labels.to(device)\n","            image_features = model.encode_image(images).float()\n","            image_features = F.normalize(image_features, dim=-1)\n","\n","            logits = image_features @ text_features_all.T\n","            accs = compute_topk_accuracy(logits, labels)\n","\n","            top1_total += accs['top1'] * images.size(0)\n","            top3_total += accs['top3'] * images.size(0)\n","            top5_total += accs['top5'] * images.size(0)\n","            total_samples += images.size(0)\n","\n","    return {\n","        'top1': top1_total / total_samples,\n","        'top3': top3_total / total_samples,\n","        'top5': top5_total / total_samples,\n","    }\n","\n","# Run zero-shot evaluation\n","zero_shot_results = compute_zero_shot_topk_accuracy(original_model, test_loader, text_features_all, device)\n","\n","print(\"\\nZero-Shot CLIP Accuracy:\")\n","print(f\"Top-1: {zero_shot_results['top1']:.2f}%\")\n","print(f\"Top-3: {zero_shot_results['top3']:.2f}%\")\n","print(f\"Top-5: {zero_shot_results['top5']:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SG_A74iYEQFl","outputId":"82a0b45a-b3f5-423e-d534-2ef559731862"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Evaluating Zero-Shot CLIP: 100%|██████████| 26/26 [00:06<00:00,  3.99it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Zero-Shot CLIP Accuracy:\n","Top-1: 63.49%\n","Top-3: 81.32%\n","Top-5: 84.98%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}