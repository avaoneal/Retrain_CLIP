{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Retraining CLIP"],"metadata":{"id":"4KStJjS-IHyh"}},{"cell_type":"markdown","source":["## **IMPORTANT NOTE**"],"metadata":{"id":"s4v99RcAHEDq"}},{"cell_type":"markdown","source":["This data set was too big to train on the entire thing. I used a sample of 50,000 images for training, 5,000 for validation, and 10,000 for testing. The full data set has about 312,000 images."],"metadata":{"id":"bECx0JRHHITm"}},{"cell_type":"markdown","source":["## Set Up"],"metadata":{"id":"9WzEZ_UCbzJ2"}},{"cell_type":"markdown","source":["Load in all our packages"],"metadata":{"id":"68YhHiFHbigh"}},{"cell_type":"code","source":["# Install necessary packages\n","!pip install ftfy regex tqdm\n","!pip install git+https://github.com/openai/CLIP.git\n","!pip install scipy"],"metadata":{"id":"Mico2OwQ_TuM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","import clip\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","from torchvision import transforms\n","from torchvision import datasets\n","from PIL import Image\n","import os\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","import shutil\n","import packaging\n","import kagglehub\n","import pandas as pd\n","import json"],"metadata":{"id":"lHMXkYSba1KH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check PyTorch version\n","# Ensure compatibility with CUDA\n","version = packaging.version.parse(torch.__version__)\n","if version > packaging.version.parse('1.7.0'):\n","    print(\"Pytorch version is above 1.7.0\")\n","    print(\"It is version:\", version)\n","else:\n","    print(\"PyTorch version is not above 1.7.0. Please Upgrade\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Am9l2tDXa5vT","outputId":"8de21989-655a-4ccf-9851-a3772f1be057"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pytorch version is above 1.7.0\n","It is version: 2.6.0+cu124\n"]}]},{"cell_type":"markdown","source":["Get the Clip Model"],"metadata":{"id":"ramFc_fIbu4i"}},{"cell_type":"code","source":["# Load CLIP model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)\n","model = model.float()"],"metadata":{"id":"UPvdMxm4a8mr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5bac3d61-3b54-4a6b-eb65-b074a6e8fd21"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|███████████████████████████████████████| 338M/338M [00:03<00:00, 93.8MiB/s]\n"]}]},{"cell_type":"markdown","source":["### Unfreeze more layers from CLIP\n","\n","By default, many pre-trained models like CLIP freeze their internal layers. This means the weights of those layers don't get updated during training. Freezing maintains the extracted features from the initial training. But if we want the model to adapt to our new data, we need to \"unfreeze\" certain layers so they can be trained."],"metadata":{"id":"ZISC2-06WdLE"}},{"cell_type":"code","source":["for name, param in model.named_parameters():\n","    # This loop goes through every parameter (weight/bias) in the CLIP model.\n","    # `name` is a string describing which layer the parameter belongs to.\n","    # `param` is the actual parameter tensor (a PyTorch object containing weights).\n","\n","    if \"visual\" in name:\n","        # Only unfreeze layers in the \"visual\" part of the model.\n","        # CLIP has two main parts: a visual encoder (for images) and a text encoder (for text).\n","        # We only want to modify the visual encoder.\n","\n","        param.requires_grad = True\n","        # This tells PyTorch: \"Yes, this parameter should be updated during training.\"\n","        # Any parameter with `requires_grad = False` will be ignored during backpropagation.\n"],"metadata":{"id":"ol9ZYKNra_mc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###  Define linear classification head\n","\n","This is a very simple MLP neural network: a single fully connected linear layer. It's used to map the output of CLIP's image encoder to a set of class predictions. Think of it like the final decision layer that says: \"I think this image is class X.\""],"metadata":{"id":"tImqfz78XJsG"}},{"cell_type":"code","source":["class LinearClassifier(nn.Module):\n","    def __init__(self, input_dim, num_classes):\n","        # Constructor for the class. Called when we create an instance of LinearClassifier.\n","        # `input_dim` is the size of the input features (from the CLIP image encoder: 512).\n","        # `num_classes` is the number of categories we want to classify\n","\n","        super(LinearClassifier, self).__init__()\n","\n","        self.fc = nn.Linear(input_dim, num_classes)\n","        # This creates the linear (fully connected) layer.\n","        # It takes a vector of size `input_dim`\n","        # and outputs a vector of size `num_classes` with values\n","        # representing the similarity of an image to each class.\n","\n","    def forward(self, image_features):\n","        # This function defines how the data flows through the model during forward propogation.\n","        # It's called automatically during training and inference.\n","\n","        return self.fc(image_features)\n","        # The output is a set of raw scores (logits) for each class.\n"],"metadata":{"id":"hiaZm425bZFU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **DATA: This is the part you edit**"],"metadata":{"id":"MqGkpdBUqLKo"}},{"cell_type":"markdown","source":["To run this re-training procedure this is the **only** part you want to edit. All necessary changes can be made here. Changes elsewhere may effect the model and make them difficult to compare."],"metadata":{"id":"WH6UfxxJmFXz"}},{"cell_type":"markdown","source":["### Ok, now we actually do this on the MS COCO data set"],"metadata":{"id":"hnUTNUhujS9c"}},{"cell_type":"markdown","source":["Load data"],"metadata":{"id":"sDqHKDmDxUJM"}},{"cell_type":"code","source":["# Download latest version\n","path = kagglehub.dataset_download(\"sabahesaraki/2017-2017\")\n","print(\"Path to dataset files:\", path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zqOiUSclqNXA","outputId":"fe17f1c9-3a2b-47cd-f8c7-63f995541b60"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading from https://www.kaggle.com/api/v1/datasets/download/sabahesaraki/2017-2017?dataset_version_number=1...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 43.7G/43.7G [05:05<00:00, 154MB/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting files...\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Path to dataset files: /root/.cache/kagglehub/datasets/sabahesaraki/2017-2017/versions/1\n"]}]},{"cell_type":"markdown","source":["Split out validation set"],"metadata":{"id":"LvZbwKwlxVLR"}},{"cell_type":"code","source":["dataset_root = os.path.join(path, 'train2017', 'train2017')\n","val_root = os.path.join(path, 'val2017', 'val2017')\n","test_root = os.path.join(path, 'test2017', 'test2017')"],"metadata":{"id":"21sGP7OGwU-M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Labels"],"metadata":{"id":"nN6UbHGqA82P"}},{"cell_type":"code","source":["idx_to_class = {\n","    \"1\": \"person\",\n","    \"2\": \"bicycle\",\n","    \"3\": \"car\",\n","    \"4\": \"motorcycle\",\n","    \"5\": \"airplane\",\n","    \"6\": \"bus\",\n","    \"7\": \"train\",\n","    \"8\": \"truck\",\n","    \"9\": \"boat\",\n","    \"10\": \"traffic light\",\n","    \"11\": \"fire hydrant\",\n","    \"13\": \"stop sign\",\n","    \"14\": \"parking meter\",\n","    \"15\": \"bench\",\n","    \"16\": \"bird\",\n","    \"17\": \"cat\",\n","    \"18\": \"dog\",\n","    \"19\": \"horse\",\n","    \"20\": \"sheep\",\n","    \"21\": \"cow\",\n","    \"22\": \"elephant\",\n","    \"23\": \"bear\",\n","    \"24\": \"zebra\",\n","    \"25\": \"giraffe\",\n","    \"27\": \"backpack\",\n","    \"28\": \"umbrella\",\n","    \"31\": \"handbag\",\n","    \"32\": \"tie\",\n","    \"33\": \"suitcase\",\n","    \"34\": \"frisbee\",\n","    \"35\": \"skis\",\n","    \"36\": \"snowboard\",\n","    \"37\": \"sports ball\",\n","    \"38\": \"kite\",\n","    \"39\": \"baseball bat\",\n","    \"40\": \"baseball glove\",\n","    \"41\": \"skateboard\",\n","    \"42\": \"surfboard\",\n","    \"43\": \"tennis racket\",\n","    \"44\": \"bottle\",\n","    \"46\": \"wine glass\",\n","    \"47\": \"cup\",\n","    \"48\": \"fork\",\n","    \"49\": \"knife\",\n","    \"50\": \"spoon\",\n","    \"51\": \"bowl\",\n","    \"52\": \"banana\",\n","    \"53\": \"apple\",\n","    \"54\": \"sandwich\",\n","    \"55\": \"orange\",\n","    \"56\": \"broccoli\",\n","    \"57\": \"carrot\",\n","    \"58\": \"hot dog\",\n","    \"59\": \"pizza\",\n","    \"60\": \"donut\",\n","    \"61\": \"cake\",\n","    \"62\": \"chair\",\n","    \"63\": \"couch\",\n","    \"64\": \"potted plant\",\n","    \"65\": \"bed\",\n","    \"67\": \"dining table\",\n","    \"70\": \"toilet\",\n","    \"72\": \"tv\",\n","    \"73\": \"laptop\",\n","    \"74\": \"mouse\",\n","    \"75\": \"remote\",\n","    \"76\": \"keyboard\",\n","    \"77\": \"cell phone\",\n","    \"78\": \"microwave\",\n","    \"79\": \"oven\",\n","    \"80\": \"toaster\",\n","    \"81\": \"sink\",\n","    \"82\": \"refrigerator\",\n","    \"84\": \"book\",\n","    \"85\": \"clock\",\n","    \"86\": \"vase\",\n","    \"87\": \"scissors\",\n","    \"88\": \"teddy bear\",\n","    \"89\": \"hair drier\",\n","    \"90\": \"toothbrush\"\n","}"],"metadata":{"id":"rw6jnW5EA6lb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Preprocess and Get Classes"],"metadata":{"id":"7y67jHcB2XeI"}},{"cell_type":"code","source":["coco_categories = sorted(idx_to_class.items(), key=lambda x: int(x[0]))  # sort by category ID\n","idx_to_class_list = [name for _, name in coco_categories]  # 0-based index to name\n","class_to_idx = {name: i for i, name in enumerate(idx_to_class_list)}     # name -> index\n","catid_to_idx = {int(coco_id): i for i, (coco_id, name) in enumerate(coco_categories)}  # category_id -> index"],"metadata":{"id":"3r4a5BUm7oOv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_annotations(annotation_path):\n","    with open(annotation_path, 'r') as f:\n","        return json.load(f)\n","\n","class CustomCOCODataset(Dataset):\n","    def __init__(self, image_dir, annotations, transform=preprocess, image_filenames=None, catid_to_idx=None, idx_to_class=None):\n","        self.image_dir = image_dir\n","        self.annotations = annotations  # {filename: category_id}\n","        self.transform = transform\n","        self.image_filenames = image_filenames if image_filenames else list(self.annotations.keys())\n","        self.catid_to_idx = catid_to_idx\n","        self.idx_to_class = idx_to_class\n","\n","    def __len__(self):\n","        return len(self.image_filenames)\n","\n","    def __getitem__(self, idx):\n","        image_filename = self.image_filenames[idx]\n","        image_path = os.path.join(self.image_dir, image_filename)\n","        image = Image.open(image_path).convert(\"RGB\")\n","\n","        category_id = int(self.annotations[image_filename])  # original COCO ID\n","        label_idx = self.catid_to_idx[category_id]  # 0-based index\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, label_idx\n","\n","# Update annotations to use string category IDs to match idx_to_class\n","def build_filename_to_label(ann_json):\n","    images = {img['id']: img['file_name'] for img in ann_json['images']}\n","    annotations = ann_json['annotations']\n","\n","    filename_to_label = {}\n","    for ann in annotations:\n","        image_id = ann['image_id']\n","        category_id = ann['category_id']\n","        filename = images[image_id]\n","\n","        # Convert category_id to string to match idx_to_class keys\n","        filename_to_label[filename] = str(category_id)\n","    return filename_to_label"],"metadata":{"id":"gazTIKtfGFkc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Paths to the annotation files\n","train_annotation_path = os.path.join(path, 'annotations_trainval2017', 'annotations', 'instances_train2017.json')\n","val_annotation_path = os.path.join(path, 'annotations_trainval2017', 'annotations', 'instances_val2017.json')\n","\n","# Load the JSON files\n","train_json = load_annotations(train_annotation_path)\n","val_json = load_annotations(val_annotation_path)\n","\n","train_annotations = build_filename_to_label(train_json)\n","val_annotations = build_filename_to_label(val_json)\n","\n","# Get image filenames\n","train_image_filenames = list(train_annotations.keys())\n","val_image_filenames = list(val_annotations.keys())"],"metadata":{"id":"PLx4daU9GJgo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split:\n","train_sample = random.sample(train_image_filenames, 50000)               # 50,000 train images\n","remaining_train = list(set(train_image_filenames) - set(train_sample))   # leftovers for test\n","test_sample = random.sample(remaining_train, 10000)                      # 10,000 test images\n","\n","val_sample = val_image_filenames  # All 5000 validation images"],"metadata":{"id":"0_9P6n6U7xr1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataset construction\n","train_dataset = CustomCOCODataset(\n","    image_dir=dataset_root,\n","    annotations=train_annotations,\n","    transform=preprocess,\n","    image_filenames=train_sample,\n","    catid_to_idx=catid_to_idx,\n","    idx_to_class=idx_to_class_list\n",")\n","\n","val_dataset = CustomCOCODataset(\n","    image_dir=val_root,\n","    annotations=val_annotations,\n","    transform=preprocess,\n","    image_filenames=val_sample,\n","    catid_to_idx=catid_to_idx,\n","    idx_to_class=idx_to_class_list\n",")\n","\n","test_dataset = CustomCOCODataset(\n","    image_dir=dataset_root,\n","    annotations=train_annotations,\n","    transform=preprocess,\n","    image_filenames=test_sample,\n","    catid_to_idx=catid_to_idx,\n","    idx_to_class=idx_to_class_list\n",")\n","\n","# Dataloaders\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"],"metadata":{"id":"HOFSEO2hqXyo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","    all_text_prompts = [f\"A photo of a {classname}\" for classname in idx_to_class_list]\n","    tokenized_texts = clip.tokenize(all_text_prompts).to(device)\n","    text_features_all = model.encode_text(tokenized_texts)\n","    text_features_all = F.normalize(text_features_all, dim=-1).float()"],"metadata":{"id":"I0Xlz5_d77Cp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Get test set set up for later"],"metadata":{"id":"jPTip_n7xhZV"}},{"cell_type":"code","source":["# COCO-style version\n","\n","# Get the class names from the test dataset (these are in correct order)\n","class_names_test = idx_to_class_list\n","\n","print(\"Class names:\", class_names_test)\n","\n","# Build natural language prompts for each class\n","all_texts = [f\"A photo of a {classname}\" for classname in class_names_test]  # Feel free to customize these prompts\n","\n","# Tokenize and encode with CLIP\n","with torch.no_grad():\n","    tokenized_texts = clip.tokenize(all_texts).to(device)\n","    text_features_all = model.encode_text(tokenized_texts)\n","    text_features_all = F.normalize(text_features_all, dim=-1).float()"],"metadata":{"id":"jvfqV9oxnhsW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bcde2e4e-fe0a-44f9-ee6e-6fa09b74f579"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Class names: ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n"]}]},{"cell_type":"markdown","source":["## **OK, STOP EDITING HERE**\n","\n","The rest of this file should work just fine without edits if you didn't change any variable names"],"metadata":{"id":"qjcXBTSjki5s"}},{"cell_type":"markdown","source":["## Let's Retrain"],"metadata":{"id":"jYoC_6-UYJ4V"}},{"cell_type":"code","source":["classifier = LinearClassifier(input_dim=512, num_classes=len(class_names)).to(device)\n","# Initializes the classifier we defined earlier\n","\n","optimizer = torch.optim.AdamW([\n","    {\"params\": model.visual.parameters(), \"lr\": 1e-6},\n","    {\"params\": classifier.parameters(), \"lr\": 1e-4}\n","], weight_decay=1e-4)\n","# We're training two parts:\n","# 1) model.visual: The vision encoder from CLIP — we fine-tune it very gently using a small learning rate (1e-6)\n","# 2) classifier: Our new linear layer — it starts from scratch, so we train it more aggressively (1e-4)\n","# AdamW is a common optimizer\n","\n","criterion = nn.CrossEntropyLoss()\n","# Cross-entropy compares the predicted scores (logits) against the true label and penalizes wrong guesses.\n","\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n","# This slowly reduces the learning rate over time in a smooth cosine curve\n","# this is a common trick to make training more stable and avoid overshooting the minimum loss.\n","\n","\n","num_epochs = 10\n","# how many times we loop through the whole dataset\n","\n","best_val_acc = 0\n","# keeps track of the best accuracy we've seen so far\n","\n","patience = 3\n","# For early stopping — we stop training if validation accuracy doesn’t improve for 3 straight epochs\n","# This trains more efficiently and prevents overfitting\n","\n","epochs_no_improve = 0\n","# how many times we've failed to beat our best accuracy\n","\n","for epoch in range(num_epochs):\n","\n","    classifier.train()\n","    # classifier.train() puts the model in training mode\n","\n","    total_loss, correct, total = 0, 0, 0\n","\n","    # Training ##################################################\n","\n","    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Train)\"):\n","\n","        images, labels = images.to(device), labels.to(device)\n","        image_features = model.encode_image(images).float()\n","        # Use CLIP’s vision model to encode the images into 512-dimension feature vectors\n","        image_features = F.normalize(image_features, dim=-1)\n","        # Normalize them (unit length) so comparisons (dot products) behave like cosine similarity\n","\n","        with torch.no_grad():\n","            clip_logits = image_features @ text_features_all.T  # (B, num_classes)\n","        # Dot product between image and text features. Gives similarity scores\n","        # (logits) between each image and all class names.\n","\n","        classifier_logits = classifier(image_features)\n","        # our classifier’s own guess — based on its trained weights\n","\n","        clip_logits = clip_logits / clip_logits.norm(dim=-1, keepdim=True)\n","        classifier_logits = classifier_logits / classifier_logits.norm(dim=-1, keepdim=True)\n","        # Normalize - ensures the same scale\n","\n","        blended_logits = 0.5 * classifier_logits + 0.5 * clip_logits\n","        # average the scores from CLIP and our linear classifier\n","\n","        loss = criterion(blended_logits, labels)\n","        # Calculate the loss from the blended prediction\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        # Clear old gradients, backpropagate new ones, and take an optimizer step\n","\n","        total_loss += loss.item()\n","        correct += (blended_logits.argmax(dim=1) == labels).sum().item()\n","        total += labels.size(0)\n","        # Count how many predictions were correct and update total loss and accuracy\n","\n","    train_acc = 100 * correct / total\n","    print(f\"Epoch {epoch+1}: Train Loss = {total_loss:.4f}, Train Acc = {train_acc:.2f}%\")\n","\n","    # Validation ################################################\n","\n","    classifier.eval()\n","    # Switch model to evaluation mode\n","\n","    correct, total = 0, 0\n","\n","    with torch.no_grad():\n","        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Val)\"):\n","            images, labels = images.to(device), labels.to(device)\n","            image_features = model.encode_image(images).float()\n","            image_features = F.normalize(image_features, dim=-1)\n","\n","            with torch.no_grad():\n","                clip_logits = image_features @ text_features_all.T\n","            classifier_logits = classifier(image_features)\n","            clip_logits = clip_logits / clip_logits.norm(dim=-1, keepdim=True)\n","            classifier_logits = classifier_logits / classifier_logits.norm(dim=-1, keepdim=True)\n","            logits = 0.5 * classifier_logits + 0.5 * clip_logits\n","            correct += (logits.argmax(dim=1) == labels).sum().item()\n","            total += labels.size(0)\n","\n","    val_acc = 100 * correct / total\n","    print(f\"Epoch {epoch+1}: Val Acc = {val_acc:.2f}%\")\n","    # Count correct predictions to compute validation accuracy\n","\n","    # Early Stopping #############################################\n","\n","    if val_acc > best_val_acc:\n","        best_val_acc = val_acc\n","        epochs_no_improve = 0\n","        torch.save(classifier.state_dict(), 'best_linear_classifier.pth')\n","        print(\"Improved validation accuracy. Saved model.\")\n","    # If we beat our best validation accuracy, save the model\n","\n","    else:\n","        epochs_no_improve += 1\n","        if epochs_no_improve >= patience:\n","            print(\"Early stopping.\")\n","            break\n","    # If we’ve gone patience epochs with no improvement, stop training early\n","\n","    scheduler.step()\n","    # Move along the cosine schedule — lower the learning rate a bit\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6O2clmHwvIkl","outputId":"879a38cb-350d-4155-ed9b-c8f21d0a5a74"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1/10 (Train): 100%|██████████| 1563/1563 [09:18<00:00,  2.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Train Loss = 6320.2912, Train Acc = 63.92%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/10 (Val): 100%|██████████| 155/155 [00:45<00:00,  3.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Val Acc = 52.26%\n","Improved validation accuracy. Saved model.\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/10 (Train): 100%|██████████| 1563/1563 [09:09<00:00,  2.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Train Loss = 6225.4611, Train Acc = 73.71%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/10 (Val): 100%|██████████| 155/155 [00:45<00:00,  3.40it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Val Acc = 51.07%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/10 (Train): 100%|██████████| 1563/1563 [09:08<00:00,  2.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3: Train Loss = 6191.5421, Train Acc = 78.06%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/10 (Val): 100%|██████████| 155/155 [00:45<00:00,  3.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3: Val Acc = 51.60%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/10 (Train): 100%|██████████| 1563/1563 [09:08<00:00,  2.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4: Train Loss = 6168.4263, Train Acc = 80.86%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/10 (Val): 100%|██████████| 155/155 [00:46<00:00,  3.37it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 4: Val Acc = 49.82%\n","Early stopping.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["## Compute Accuracy with Newly Trained Model"],"metadata":{"id":"wDH6f6RZ1HGc"}},{"cell_type":"code","source":["def compute_topk_accuracy(logits, labels, topk=(1, 3, 5)):\n","    max_k = max(topk)\n","    batch_size = labels.size(0)\n","\n","    _, pred = logits.topk(max_k, dim=1, largest=True, sorted=True)\n","    pred = pred.t()\n","    correct = pred.eq(labels.view(1, -1).expand_as(pred))\n","\n","    topk_accs = {}\n","    for k in topk:\n","        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n","        topk_accs[f\"top{k}\"] = (correct_k / batch_size).item() * 100.0\n","\n","    return topk_accs\n","\n","# Evaluate fine-tuned classifier\n","classifier.eval()\n","top1_total, top3_total, top5_total, total_samples = 0, 0, 0, 0\n","\n","with torch.no_grad():\n","    for images, labels in tqdm(test_loader, desc=\"Evaluating Fine-tuned Classifier\"):\n","        images, labels = images.to(device), labels.to(device)\n","        image_features = model.encode_image(images).float()\n","        image_features = F.normalize(image_features, dim=-1)\n","\n","        logits = classifier(image_features)\n","        accs = compute_topk_accuracy(logits, labels)\n","\n","        top1_total += accs['top1'] * images.size(0)\n","        top3_total += accs['top3'] * images.size(0)\n","        top5_total += accs['top5'] * images.size(0)\n","        total_samples += images.size(0)\n","\n","print(f\"\\nFine-tuned Classifier Accuracy:\")\n","print(f\"Top-1: {top1_total / total_samples:.2f}%\")\n","print(f\"Top-3: {top3_total / total_samples:.2f}%\")\n","print(f\"Top-5: {top5_total / total_samples:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PqmSJM0h1bqP","outputId":"f2519e95-571d-4a42-96f5-839360d24bff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Evaluating Fine-tuned Classifier: 100%|██████████| 313/313 [01:34<00:00,  3.31it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Fine-tuned Classifier Accuracy:\n","Top-1: 67.23%\n","Top-3: 80.79%\n","Top-5: 84.57%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["## Compare To Zero Shot Accuracy"],"metadata":{"id":"owv0DDDIxp5s"}},{"cell_type":"code","source":["original_model = clip.load(\"ViT-B/32\", device=device)[0].float().eval()\n","# Tokenize and encode with CLIP\n","with torch.no_grad():\n","    tokenized_texts = clip.tokenize(all_texts).to(device)\n","    text_features_all = model.encode_text(tokenized_texts)\n","    text_features_all = F.normalize(text_features_all, dim=-1).float()\n","\n","def compute_zero_shot_topk_accuracy(model, image_loader, text_features_all, device):\n","    model.eval()\n","    text_features_all = F.normalize(text_features_all, dim=-1)\n","\n","    top1_total, top3_total, top5_total, total_samples = 0, 0, 0, 0\n","\n","    with torch.no_grad():\n","        for images, labels in tqdm(image_loader, desc=\"Evaluating Zero-Shot CLIP\"):\n","            images, labels = images.to(device), labels.to(device)\n","            image_features = model.encode_image(images).float()\n","            image_features = F.normalize(image_features, dim=-1)\n","\n","            logits = image_features @ text_features_all.T\n","            accs = compute_topk_accuracy(logits, labels)\n","\n","            top1_total += accs['top1'] * images.size(0)\n","            top3_total += accs['top3'] * images.size(0)\n","            top5_total += accs['top5'] * images.size(0)\n","            total_samples += images.size(0)\n","\n","    return {\n","        'top1': top1_total / total_samples,\n","        'top3': top3_total / total_samples,\n","        'top5': top5_total / total_samples,\n","    }\n","\n","# Run zero-shot evaluation\n","zero_shot_results = compute_zero_shot_topk_accuracy(original_model, test_loader, text_features_all, device)\n","\n","print(\"\\nZero-Shot CLIP Accuracy:\")\n","print(f\"Top-1: {zero_shot_results['top1']:.2f}%\")\n","print(f\"Top-3: {zero_shot_results['top3']:.2f}%\")\n","print(f\"Top-5: {zero_shot_results['top5']:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SG_A74iYEQFl","outputId":"a61743e2-6853-42d1-df4a-4c0722773580"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Evaluating Zero-Shot CLIP: 100%|██████████| 313/313 [01:33<00:00,  3.35it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Zero-Shot CLIP Accuracy:\n","Top-1: 36.94%\n","Top-3: 54.41%\n","Top-5: 64.12%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}