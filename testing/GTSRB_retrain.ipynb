{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Retraining CLIP"],"metadata":{"id":"4KStJjS-IHyh"}},{"cell_type":"markdown","source":["## Set Up"],"metadata":{"id":"9WzEZ_UCbzJ2"}},{"cell_type":"markdown","source":["Load in all our packages"],"metadata":{"id":"68YhHiFHbigh"}},{"cell_type":"code","source":["# Install necessary packages\n","!pip install ftfy regex tqdm\n","!pip install git+https://github.com/openai/CLIP.git\n","!pip install scipy\n","\n","import clip\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","from torchvision import transforms\n","from torchvision import datasets\n","from PIL import Image\n","import os\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","import shutil\n","import packaging\n","import kagglehub\n","import pandas as pd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lHMXkYSba1KH","outputId":"fe0d2b3c-1a87-4ec9-d3dc-efc4f9bc7a9d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ftfy\n","  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n","Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: ftfy\n","Successfully installed ftfy-6.3.1\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-ecsgolh2\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-ecsgolh2\n","  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->clip==1.0)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->clip==1.0)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->clip==1.0)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->clip==1.0)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->clip==1.0)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->clip==1.0)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch->clip==1.0)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->clip==1.0)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->clip==1.0)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->clip==1.0)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=253fc2dfead567002c25546e0b1dab0538322f5e61bf09c3d8690639ff611c38\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-pjoq_28_/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n","Successfully built clip\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, clip\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed clip-1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.14.1)\n","Requirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (2.0.2)\n"]}]},{"cell_type":"code","source":["# Check PyTorch version\n","# Ensure compatibility with CUDA\n","version = packaging.version.parse(torch.__version__)\n","if version > packaging.version.parse('1.7.0'):\n","    print(\"Pytorch version is above 1.7.0\")\n","    print(\"It is version:\", version)\n","else:\n","    print(\"PyTorch version is not above 1.7.0. Please Upgrade\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Am9l2tDXa5vT","outputId":"0f7b4fab-7267-4b2e-8c6c-c1a5ef1639c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pytorch version is above 1.7.0\n","It is version: 2.6.0+cu124\n"]}]},{"cell_type":"markdown","source":["Get the Clip Model"],"metadata":{"id":"ramFc_fIbu4i"}},{"cell_type":"code","source":["# Load CLIP model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)\n","model = model.float()"],"metadata":{"id":"UPvdMxm4a8mr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9d71ff9c-d207-4faf-ad4b-657f222d0c9c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|███████████████████████████████████████| 338M/338M [00:06<00:00, 50.9MiB/s]\n"]}]},{"cell_type":"markdown","source":["### Unfreeze more layers from CLIP\n","\n","By default, many pre-trained models like CLIP freeze their internal layers. This means the weights of those layers don't get updated during training. Freezing maintains the extracted features from the initial training. But if we want the model to adapt to our new data, we need to \"unfreeze\" certain layers so they can be trained."],"metadata":{"id":"ZISC2-06WdLE"}},{"cell_type":"code","source":["for name, param in model.named_parameters():\n","    # This loop goes through every parameter (weight/bias) in the CLIP model.\n","    # `name` is a string describing which layer the parameter belongs to.\n","    # `param` is the actual parameter tensor (a PyTorch object containing weights).\n","\n","    if \"visual\" in name:\n","        # Only unfreeze layers in the \"visual\" part of the model.\n","        # CLIP has two main parts: a visual encoder (for images) and a text encoder (for text).\n","        # We only want to modify the visual encoder.\n","\n","        param.requires_grad = True\n","        # This tells PyTorch: \"Yes, this parameter should be updated during training.\"\n","        # Any parameter with `requires_grad = False` will be ignored during backpropagation."],"metadata":{"id":"ol9ZYKNra_mc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###  Define linear classification head\n","\n","This is a very simple MLP neural network: a single fully connected linear layer. It's used to map the output of CLIP's image encoder to a set of class predictions. Think of it like the final decision layer that says: \"I think this image is class X.\""],"metadata":{"id":"tImqfz78XJsG"}},{"cell_type":"code","source":["class LinearClassifier(nn.Module):\n","    def __init__(self, input_dim, num_classes):\n","        # Constructor for the class. Called when we create an instance of LinearClassifier.\n","        # `input_dim` is the size of the input features (from the CLIP image encoder: 512).\n","        # `num_classes` is the number of categories we want to classify\n","\n","        super(LinearClassifier, self).__init__()\n","\n","        self.fc = nn.Linear(input_dim, num_classes)\n","        # This creates the linear (fully connected) layer.\n","        # It takes a vector of size `input_dim`\n","        # and outputs a vector of size `num_classes` with values\n","        # representing the similarity of an image to each class.\n","\n","    def forward(self, image_features):\n","        # This function defines how the data flows through the model during forward propogation.\n","        # It's called automatically during training and inference.\n","\n","        return self.fc(image_features)\n","        # The output is a set of raw scores (logits) for each class.\n"],"metadata":{"id":"hiaZm425bZFU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **DATA: This is the part you edit**"],"metadata":{"id":"MqGkpdBUqLKo"}},{"cell_type":"markdown","source":["To run this re-training procedure this is the **only** part you want to edit. All necessary changes can be made here. Changes elsewhere may effect the model and make them difficult to compare."],"metadata":{"id":"WH6UfxxJmFXz"}},{"cell_type":"markdown","source":["### Now we actually do this on GTSRB data set"],"metadata":{"id":"hnUTNUhujS9c"}},{"cell_type":"markdown","source":["Load data"],"metadata":{"id":"sDqHKDmDxUJM"}},{"cell_type":"code","source":["# Download latest version\n","path = kagglehub.dataset_download(\"meowmeowmeowmeowmeow/gtsrb-german-traffic-sign\") # Change this to your data set\n","print(os.listdir(path))\n","\n","# Load CSV files for train and test\n","train_dir = os.path.join(path, 'Train.csv') # Change to your train folder\n","test_dir = os.path.join(path, 'Test.csv') # Change to your test folder\n","\n","train_df = pd.read_csv(train_dir)\n","test_df = pd.read_csv(test_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zqOiUSclqNXA","outputId":"0bd2402d-2e11-4603-f086-f2c403cf1c8a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Meta', 'meta', 'Meta.csv', 'Train.csv', 'Test.csv', 'Test', 'test', 'Train', 'train']\n"]}]},{"cell_type":"markdown","source":["Define dataset class to handle images and class connections"],"metadata":{"id":"yIxNM_NYyVF9"}},{"cell_type":"code","source":["class GTSRBDataset(Dataset):\n","    def __init__(self, dataframe, root_dir, transform=None):\n","        \"\"\"\n","        Args:\n","            dataframe (pd.DataFrame): Dataframe with image paths and labels.\n","            root_dir (str): Directory where the \"Train\" folder is located.\n","            transform (callable, optional): Optional transform to be applied on a sample.\n","        \"\"\"\n","        self.data = dataframe\n","        self.root_dir = root_dir  # Directory where the \"Train\" folder is located\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        # Correct the path construction to point directly to the image file\n","        img_name = os.path.join(self.root_dir, self.data.iloc[idx]['Path'])  # 'Path' is the filename, change to the column containing file names in csv\n","        image = Image.open(img_name).convert(\"RGB\")\n","\n","        # Accessing the label from the 'ClassId' column\n","        label = int(self.data.iloc[idx]['ClassId']) # 'ClassId' is the class names, change to the column containing class names in csv\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, label"],"metadata":{"id":"Ka6EY_hgyVPG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Split out validation set"],"metadata":{"id":"LvZbwKwlxVLR"}},{"cell_type":"code","source":["# Split the training dataset into train and validation sets\n","train_data, val_data = train_test_split(train_df, test_size=0.2, random_state=42)"],"metadata":{"id":"21sGP7OGwU-M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Preprocess"],"metadata":{"id":"_O1QUnYtxZLR"}},{"cell_type":"code","source":["# Create train and validation datasets\n","train_dataset = GTSRBDataset(dataframe=train_data, root_dir=path, transform=preprocess)\n","val_dataset = GTSRBDataset(dataframe=val_data, root_dir=path, transform=preprocess)\n","test_dataset = GTSRBDataset(dataframe=test_df, root_dir=path, transform=preprocess)\n","\n","# Create DataLoader for train, validation, and test sets\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"],"metadata":{"id":"HOFSEO2hqXyo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Get classes"],"metadata":{"id":"xuTwmMQhxdoN"}},{"cell_type":"code","source":["# ClassId to human-readable traffic sign names\n","# This data only has numeric labels so I added text labels\n","# No need to do this if your data already has text labels\n","class_id_to_name = {\n","    0: \"speed limit 20 km/h\",\n","    1: \"speed limit 30 km/h\",\n","    2: \"speed limit 50 km/h\",\n","    3: \"speed limit 60 km/h\",\n","    4: \"speed limit 70 km/h\",\n","    5: \"speed limit 80 km/h\",\n","    6: \"end of speed limit 80 km/h\",\n","    7: \"speed limit 100 km/h\",\n","    8: \"speed limit 120 km/h\",\n","    9: \"no passing\",\n","    10: \"no passing for vehicles over 3.5 metric tons\",\n","    11: \"right-of-way at the next intersection\",\n","    12: \"priority road\",\n","    13: \"yield\",\n","    14: \"stop\",\n","    15: \"no vehicles\",\n","    16: \"vehicles over 3.5 metric tons prohibited\",\n","    17: \"no entry\",\n","    18: \"general caution\",\n","    19: \"dangerous curve to the left\",\n","    20: \"dangerous curve to the right\",\n","    21: \"double curve\",\n","    22: \"bumpy road\",\n","    23: \"slippery road\",\n","    24: \"road narrows on the right\",\n","    25: \"road work\",\n","    26: \"traffic signals\",\n","    27: \"pedestrians\",\n","    28: \"children crossing\",\n","    29: \"bicycles crossing\",\n","    30: \"beware of ice/snow\",\n","    31: \"wild animals crossing\",\n","    32: \"end of all speed and passing limits\",\n","    33: \"turn right ahead\",\n","    34: \"turn left ahead\",\n","    35: \"ahead only\",\n","    36: \"go straight or right\",\n","    37: \"go straight or left\",\n","    38: \"keep right\",\n","    39: \"keep left\",\n","    40: \"roundabout mandatory\",\n","    41: \"end of no passing\",\n","    42: \"end of no passing by vehicles over 3.5 metric tons\"\n","}\n","\n","class_ids = sorted(train_df['ClassId'].unique()) # 'ClassId' is the class names, change to the column containing class names in csv\n","class_names = [class_id_to_name[i] for i in class_ids] # Don't do this if your data is already in text format\n","# If your labels are in text format already, change the class_ids variable name to class_names and delete the second line\n","\n","all_texts = [f\"A traffic sign that means {name}\" for name in class_names] # Feel free to change prompts here\n","tokenized_texts = clip.tokenize(all_texts).to(device)\n","with torch.no_grad():\n","    text_features_all = model.encode_text(tokenized_texts)\n","    text_features_all = F.normalize(text_features_all, dim=-1).float()"],"metadata":{"id":"My0rOfwSrP6h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Get test set set up for later"],"metadata":{"id":"jPTip_n7xhZV"}},{"cell_type":"code","source":["test_dataset = GTSRBDataset(dataframe=test_df, root_dir=path, transform=preprocess)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","# Use the same GTSRBDataset for the test set\n","\n","class_ids = sorted(test_df['ClassId'].unique()) # 'ClassId' is the class names, change to the column containing class names in csv\n","class_names_test = [class_id_to_name[i] for i in class_ids] # Don't do this if your data is already in text format\n","print(\"Class names:\", class_names_test)\n","# If your labels are in text format already, change the class_ids variable name to class_names and delete the second line\n","\n","all_texts = [f\"A photo of a road sign showing {classname}\" for classname in class_names_test] # Feel free to change prompts here\n","tokenized_texts = clip.tokenize(all_texts).to(device)\n","with torch.no_grad():\n","    text_features_all = model.encode_text(tokenized_texts)  # Shape: (num_classes, 512)\n","    text_features_all = F.normalize(text_features_all, dim=-1).float()"],"metadata":{"id":"jvfqV9oxnhsW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2e005987-aeb6-455f-92a8-7924c6bb5da7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Class names: ['speed limit 20 km/h', 'speed limit 30 km/h', 'speed limit 50 km/h', 'speed limit 60 km/h', 'speed limit 70 km/h', 'speed limit 80 km/h', 'end of speed limit 80 km/h', 'speed limit 100 km/h', 'speed limit 120 km/h', 'no passing', 'no passing for vehicles over 3.5 metric tons', 'right-of-way at the next intersection', 'priority road', 'yield', 'stop', 'no vehicles', 'vehicles over 3.5 metric tons prohibited', 'no entry', 'general caution', 'dangerous curve to the left', 'dangerous curve to the right', 'double curve', 'bumpy road', 'slippery road', 'road narrows on the right', 'road work', 'traffic signals', 'pedestrians', 'children crossing', 'bicycles crossing', 'beware of ice/snow', 'wild animals crossing', 'end of all speed and passing limits', 'turn right ahead', 'turn left ahead', 'ahead only', 'go straight or right', 'go straight or left', 'keep right', 'keep left', 'roundabout mandatory', 'end of no passing', 'end of no passing by vehicles over 3.5 metric tons']\n"]}]},{"cell_type":"markdown","source":["## **OK, STOP EDITING HERE**\n","\n","The rest of this file should work just fine without edits if you didn't change any variable names"],"metadata":{"id":"qjcXBTSjki5s"}},{"cell_type":"markdown","source":["## Let's Retrain"],"metadata":{"id":"jYoC_6-UYJ4V"}},{"cell_type":"code","source":["classifier = LinearClassifier(input_dim=512, num_classes=len(class_names)).to(device)\n","# Initializes the classifier we defined earlier\n","\n","optimizer = torch.optim.AdamW([\n","    {\"params\": model.visual.parameters(), \"lr\": 1e-6},\n","    {\"params\": classifier.parameters(), \"lr\": 1e-4}\n","], weight_decay=1e-4)\n","# We're training two parts:\n","# 1) model.visual: The vision encoder from CLIP — we fine-tune it very gently using a small learning rate (1e-6)\n","# 2) classifier: Our new linear layer — it starts from scratch, so we train it more aggressively (1e-4)\n","# AdamW is a common optimizer\n","\n","criterion = nn.CrossEntropyLoss()\n","# Cross-entropy compares the predicted scores (logits) against the true label and penalizes wrong guesses.\n","\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n","# This slowly reduces the learning rate over time in a smooth cosine curve\n","# this is a common trick to make training more stable and avoid overshooting the minimum loss.\n","\n","\n","num_epochs = 10\n","# how many times we loop through the whole dataset\n","\n","best_val_acc = 0\n","# keeps track of the best accuracy we've seen so far\n","\n","patience = 3\n","# For early stopping — we stop training if validation accuracy doesn’t improve for 3 straight epochs\n","# This trains more efficiently and prevents overfitting\n","\n","epochs_no_improve = 0\n","# how many times we've failed to beat our best accuracy\n","\n","for epoch in range(num_epochs):\n","\n","    classifier.train()\n","    # classifier.train() puts the model in training mode\n","\n","    total_loss, correct, total = 0, 0, 0\n","\n","    # Training ##################################################\n","\n","    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Train)\"):\n","\n","        images, labels = images.to(device), labels.to(device)\n","        image_features = model.encode_image(images).float()\n","        # Use CLIP’s vision model to encode the images into 512-dimension feature vectors\n","        image_features = F.normalize(image_features, dim=-1)\n","        # Normalize them (unit length) so comparisons (dot products) behave like cosine similarity\n","\n","        with torch.no_grad():\n","            clip_logits = image_features @ text_features_all.T  # (B, num_classes)\n","        # Dot product between image and text features. Gives similarity scores\n","        # (logits) between each image and all class names.\n","\n","        classifier_logits = classifier(image_features)\n","        # our classifier’s own guess — based on its trained weights\n","\n","        clip_logits = clip_logits / clip_logits.norm(dim=-1, keepdim=True)\n","        classifier_logits = classifier_logits / classifier_logits.norm(dim=-1, keepdim=True)\n","        # Normalize - ensures the same scale\n","\n","        blended_logits = 0.5 * classifier_logits + 0.5 * clip_logits\n","        # average the scores from CLIP and our linear classifier\n","\n","        loss = criterion(blended_logits, labels)\n","        # Calculate the loss from the blended prediction\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        # Clear old gradients, backpropagate new ones, and take an optimizer step\n","\n","        total_loss += loss.item()\n","        correct += (blended_logits.argmax(dim=1) == labels).sum().item()\n","        total += labels.size(0)\n","        # Count how many predictions were correct and update total loss and accuracy\n","\n","    train_acc = 100 * correct / total\n","    print(f\"Epoch {epoch+1}: Train Loss = {total_loss:.4f}, Train Acc = {train_acc:.2f}%\")\n","\n","    # Validation ################################################\n","\n","    classifier.eval()\n","    # Switch model to evaluation mode\n","\n","    correct, total = 0, 0\n","\n","    with torch.no_grad():\n","        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Val)\"):\n","            images, labels = images.to(device), labels.to(device)\n","            image_features = model.encode_image(images).float()\n","            image_features = F.normalize(image_features, dim=-1)\n","\n","            with torch.no_grad():\n","                clip_logits = image_features @ text_features_all.T\n","            classifier_logits = classifier(image_features)\n","            clip_logits = clip_logits / clip_logits.norm(dim=-1, keepdim=True)\n","            classifier_logits = classifier_logits / classifier_logits.norm(dim=-1, keepdim=True)\n","            logits = 0.5 * classifier_logits + 0.5 * clip_logits\n","            correct += (logits.argmax(dim=1) == labels).sum().item()\n","            total += labels.size(0)\n","\n","    val_acc = 100 * correct / total\n","    print(f\"Epoch {epoch+1}: Val Acc = {val_acc:.2f}%\")\n","    # Count correct predictions to compute validation accuracy\n","\n","    # Early Stopping #############################################\n","\n","    if val_acc > best_val_acc:\n","        best_val_acc = val_acc\n","        epochs_no_improve = 0\n","        torch.save(classifier.state_dict(), 'best_linear_classifier.pth')\n","        print(\"Improved validation accuracy. Saved model.\")\n","    # If we beat our best validation accuracy, save the model\n","\n","    else:\n","        epochs_no_improve += 1\n","        if epochs_no_improve >= patience:\n","            print(\"Early stopping.\")\n","            break\n","    # If we’ve gone patience epochs with no improvement, stop training early\n","\n","    scheduler.step()\n","    # Move along the cosine schedule — lower the learning rate a bit\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6O2clmHwvIkl","outputId":"033df896-9c49-409a-8113-993dff15ca99"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1/10 (Train): 100%|██████████| 981/981 [06:04<00:00,  2.69it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Train Loss = 3276.9511, Train Acc = 89.30%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/10 (Val): 100%|██████████| 246/246 [01:16<00:00,  3.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Val Acc = 99.50%\n","Improved validation accuracy. Saved model.\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/10 (Train): 100%|██████████| 981/981 [02:23<00:00,  6.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Train Loss = 3201.0141, Train Acc = 99.77%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/10 (Val): 100%|██████████| 246/246 [00:25<00:00,  9.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Val Acc = 99.76%\n","Improved validation accuracy. Saved model.\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/10 (Train): 100%|██████████| 981/981 [02:23<00:00,  6.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3: Train Loss = 3196.6524, Train Acc = 99.97%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/10 (Val): 100%|██████████| 246/246 [00:25<00:00,  9.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3: Val Acc = 99.89%\n","Improved validation accuracy. Saved model.\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/10 (Train): 100%|██████████| 981/981 [02:23<00:00,  6.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4: Train Loss = 3195.4229, Train Acc = 99.97%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/10 (Val): 100%|██████████| 246/246 [00:25<00:00,  9.58it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4: Val Acc = 99.81%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/10 (Train): 100%|██████████| 981/981 [02:23<00:00,  6.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5: Train Loss = 3194.8895, Train Acc = 99.98%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/10 (Val): 100%|██████████| 246/246 [00:25<00:00,  9.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5: Val Acc = 99.78%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/10 (Train): 100%|██████████| 981/981 [02:22<00:00,  6.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6: Train Loss = 3194.4884, Train Acc = 100.00%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/10 (Val): 100%|██████████| 246/246 [00:25<00:00,  9.66it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 6: Val Acc = 99.83%\n","Early stopping.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["## Compute Accuracy with Newly Trained Model"],"metadata":{"id":"wDH6f6RZ1HGc"}},{"cell_type":"code","source":["def compute_topk_accuracy(logits, labels, topk=(1, 3, 5)):\n","    max_k = max(topk)\n","    batch_size = labels.size(0)\n","\n","    _, pred = logits.topk(max_k, dim=1, largest=True, sorted=True)\n","    pred = pred.t()\n","    correct = pred.eq(labels.view(1, -1).expand_as(pred))\n","\n","    topk_accs = {}\n","    for k in topk:\n","        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n","        topk_accs[f\"top{k}\"] = (correct_k / batch_size).item() * 100.0\n","\n","    return topk_accs\n","\n","# Evaluate fine-tuned classifier\n","classifier.eval()\n","top1_total, top3_total, top5_total, total_samples = 0, 0, 0, 0\n","\n","with torch.no_grad():\n","    for images, labels in tqdm(test_loader, desc=\"Evaluating Fine-tuned Classifier\"):\n","        images, labels = images.to(device), labels.to(device)\n","        image_features = model.encode_image(images).float()\n","        image_features = F.normalize(image_features, dim=-1)\n","\n","        logits = classifier(image_features)\n","        accs = compute_topk_accuracy(logits, labels)\n","\n","        top1_total += accs['top1'] * images.size(0)\n","        top3_total += accs['top3'] * images.size(0)\n","        top5_total += accs['top5'] * images.size(0)\n","        total_samples += images.size(0)\n","\n","print(f\"\\nFine-tuned Classifier Accuracy:\")\n","print(f\"Top-1: {top1_total / total_samples:.2f}%\")\n","print(f\"Top-3: {top3_total / total_samples:.2f}%\")\n","print(f\"Top-5: {top5_total / total_samples:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PqmSJM0h1bqP","outputId":"1a5cb6bb-5a7d-43d4-b90a-1cb9d85c3ce1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Evaluating Fine-tuned Classifier: 100%|██████████| 395/395 [00:42<00:00,  9.38it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Fine-tuned Classifier Accuracy:\n","Top-1: 98.63%\n","Top-3: 99.58%\n","Top-5: 99.75%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["## Compare To Zero Shot Accuracy"],"metadata":{"id":"owv0DDDIxp5s"}},{"cell_type":"code","source":["original_model = clip.load(\"ViT-B/32\", device=device)[0].float().eval()\n","with torch.no_grad():\n","    tokenized_texts = clip.tokenize([f\"A photo of a {classname}\" for classname in class_names_test]).to(device)\n","    text_features_all = original_model.encode_text(tokenized_texts)\n","    text_features_all = F.normalize(text_features_all, dim=-1).float()\n","\n","def compute_zero_shot_topk_accuracy(model, image_loader, text_features_all, device):\n","    model.eval()\n","    text_features_all = F.normalize(text_features_all, dim=-1)\n","\n","    top1_total, top3_total, top5_total, total_samples = 0, 0, 0, 0\n","\n","    with torch.no_grad():\n","        for images, labels in tqdm(image_loader, desc=\"Evaluating Zero-Shot CLIP\"):\n","            images, labels = images.to(device), labels.to(device)\n","            image_features = model.encode_image(images).float()\n","            image_features = F.normalize(image_features, dim=-1)\n","\n","            logits = image_features @ text_features_all.T\n","            accs = compute_topk_accuracy(logits, labels)\n","\n","            top1_total += accs['top1'] * images.size(0)\n","            top3_total += accs['top3'] * images.size(0)\n","            top5_total += accs['top5'] * images.size(0)\n","            total_samples += images.size(0)\n","\n","    return {\n","        'top1': top1_total / total_samples,\n","        'top3': top3_total / total_samples,\n","        'top5': top5_total / total_samples,\n","    }\n","\n","# Run zero-shot evaluation\n","zero_shot_results = compute_zero_shot_topk_accuracy(original_model, test_loader, text_features_all, device)\n","\n","print(\"\\nZero-Shot CLIP Accuracy:\")\n","print(f\"Top-1: {zero_shot_results['top1']:.2f}%\")\n","print(f\"Top-3: {zero_shot_results['top3']:.2f}%\")\n","print(f\"Top-5: {zero_shot_results['top5']:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SG_A74iYEQFl","outputId":"4d43fafc-1427-48d2-dded-dac379fbe087"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Evaluating Zero-Shot CLIP: 100%|██████████| 395/395 [00:42<00:00,  9.28it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Zero-Shot CLIP Accuracy:\n","Top-1: 28.61%\n","Top-3: 39.03%\n","Top-5: 44.17%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}