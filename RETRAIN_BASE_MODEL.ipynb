{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","collapsed_sections":["tWc9dvdOmbxs","ZGft7tJJmf0G"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Retraining CLIP"],"metadata":{"id":"4KStJjS-IHyh"}},{"cell_type":"markdown","source":["## Disscussion"],"metadata":{"id":"RtqVLtHyf7K-"}},{"cell_type":"markdown","source":["This is what the original training procedure of the CLIP model looks like:\n","\n","![Alt text](https://raw.githubusercontent.com/avaoneal/Retrain_CLIP/main/diagrams/base.drawio.png)\n","\n","We had to try several types of retraining and ensembling methods before one was successful. I will briefly discuss what we tried here.\n","\n","1. Firstly, we tried to retrain the transformer models.\n","\n","  The parts we retrained are in orange and the parts we froze are in blue.\n","\n","  ![Alt text](https://raw.githubusercontent.com/avaoneal/Retrain_CLIP/main/diagrams/model1.drawio.png)\n","\n","  This was because we believed that adding convolutional layers would aid the model in recognizing fine-tuned details and creating a more context-specific set of text embeddings would aid in classification process. However, this method was unsuccessful. Additional convolutional layers actually began to increase the overfitting problem we encountered later in the embedding space as we added layers, and the additional transformer had no profound impact on performance. This helped us recognize where issue of missclassification was originating; in the embedding space.\n","\n","2. Secondly, we tried to add a transformer classifier head.\n","\n","  The parts we retrained are in orange and the parts we froze are in blue.\n","\n","  ![Alt text](https://raw.githubusercontent.com/avaoneal/Retrain_CLIP/main/diagrams/model2.drawio.png)\n","\n","  This was because we believed that adding classifier after the contrastive learning steps would fine-tune the selection of text we chose from, and pick up on more details in images that can be used to distinguish similar classes. However, this method was also unsuccessful because the largest problem CLIP experiences when classifying is overfitting. The transformer's complex architecture only increased this problem, even when using dropout, schedulers, low learning rates, and other overclassification prevention techniques. It continued to model only towards our train data, instead of reducing the overfitting from contrastive learning because the transformer only reaffirmed CLIP's overfit decision. This showed us that we needed to simplify the architecture and use a structure that could help widen the boandaries between classes.\n","\n","3. Finally, we landed on this successful architecture of an added MLP classifier head.\n","\n","  The parts we retrained are in orange and the parts we froze are in blue.\n","\n","  ![Alt text](https://raw.githubusercontent.com/avaoneal/Retrain_CLIP/main/diagrams/model3.drawio.png)\n","\n","  This was because the simpler arcitchture allows us to limit the power of contrastive learning, so that we can cut down on overfitting. We used a single linear layer to find logits for each class. We unfroze some of the ViT layers, and used a very gentle learning rate to update the weights from the vision encoder, while training our MLP layer more agressively. We evaluated with cross entropy; we used an adam optimizer, cosine scheduler, and early stopping for efficiency and to prevent vanishing gradients. We used blended logits to evaluate, averaging the scores from zero-shot CLIP and our linear classifier, to retain the power of CLIP without overfitting. This architecture has shown success.\n","\n"],"metadata":{"id":"yRap464r5HDR"}},{"cell_type":"markdown","source":["## Set Up"],"metadata":{"id":"9WzEZ_UCbzJ2"}},{"cell_type":"markdown","source":["Load in all our packages"],"metadata":{"id":"68YhHiFHbigh"}},{"cell_type":"code","source":["# Install necessary packages\n","!pip install ftfy regex tqdm\n","!pip install git+https://github.com/openai/CLIP.git\n","!pip install scipy\n","\n","import clip\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","from torchvision import transforms\n","from torchvision import datasets\n","from PIL import Image\n","import os\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","import shutil\n","import packaging\n","import kagglehub\n","import pandas as pd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lHMXkYSba1KH","outputId":"7e5a8268-b478-41dd-96f4-f8dbb56b72f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ftfy\n","  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n","Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: ftfy\n","Successfully installed ftfy-6.3.1\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-xf8uds_h\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-xf8uds_h\n","  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->clip==1.0)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->clip==1.0)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->clip==1.0)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->clip==1.0)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->clip==1.0)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->clip==1.0)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch->clip==1.0)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->clip==1.0)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->clip==1.0)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->clip==1.0)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=2b05909d9ab33ddd9966af9421bbc289ed9e5ad7ff3141c81cdc45e316dda8c2\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-7xpfcw2h/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n","Successfully built clip\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, clip\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed clip-1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.14.1)\n","Requirement already satisfied: numpy<2.3,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (2.0.2)\n"]}]},{"cell_type":"code","source":["# Check PyTorch version\n","# Ensure compatibility with CUDA\n","version = packaging.version.parse(torch.__version__)\n","if version > packaging.version.parse('1.7.0'):\n","    print(\"Pytorch version is above 1.7.0\")\n","    print(\"It is version:\", version)\n","else:\n","    print(\"PyTorch version is not above 1.7.0. Please Upgrade\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Am9l2tDXa5vT","outputId":"f38b17cd-9738-421c-b083-2e7834a35509"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pytorch version is above 1.7.0\n","It is version: 2.6.0+cu124\n"]}]},{"cell_type":"markdown","source":["Get the Clip Model"],"metadata":{"id":"ramFc_fIbu4i"}},{"cell_type":"code","source":["# Load CLIP model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)\n","model = model.float()"],"metadata":{"id":"UPvdMxm4a8mr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0e1ffcf4-9254-4ea6-a040-752079354482"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|███████████████████████████████████████| 338M/338M [00:10<00:00, 32.9MiB/s]\n"]}]},{"cell_type":"markdown","source":["### Unfreeze more layers from CLIP\n","\n","By default, many pre-trained models like CLIP freeze their internal layers. This means the weights of those layers don't get updated during training. Freezing maintains the extracted features from the initial training. But if we want the model to adapt to our new data, we need to \"unfreeze\" certain layers so they can be trained."],"metadata":{"id":"ZISC2-06WdLE"}},{"cell_type":"code","source":["for name, param in model.named_parameters():\n","    # This loop goes through every parameter (weight/bias) in the CLIP model.\n","    # `name` is a string describing which layer the parameter belongs to.\n","    # `param` is the actual parameter tensor (a PyTorch object containing weights).\n","\n","    if \"visual\" in name:\n","        # Only unfreeze layers in the \"visual\" part of the model.\n","        # CLIP has two main parts: a visual encoder (for images) and a text encoder (for text).\n","        # We only want to modify the visual encoder.\n","\n","        param.requires_grad = True\n","        # This tells PyTorch: \"Yes, this parameter should be updated during training.\"\n","        # Any parameter with `requires_grad = False` will be ignored during backpropagation.\n"],"metadata":{"id":"ol9ZYKNra_mc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###  Define linear classification head\n","\n","This is a very simple MLP neural network: a single fully connected linear layer. It's used to map the output of CLIP's image encoder to a set of class predictions. Think of it like the final decision layer that says: \"I think this image is class X.\""],"metadata":{"id":"tImqfz78XJsG"}},{"cell_type":"code","source":["class LinearClassifier(nn.Module):\n","    def __init__(self, input_dim, num_classes):\n","        # Constructor for the class. Called when we create an instance of LinearClassifier.\n","        # `input_dim` is the size of the input features (from the CLIP image encoder: 512).\n","        # `num_classes` is the number of categories we want to classify\n","\n","        super(LinearClassifier, self).__init__()\n","\n","        self.fc = nn.Linear(input_dim, num_classes)\n","        # This creates the linear (fully connected) layer.\n","        # It takes a vector of size `input_dim`\n","        # and outputs a vector of size `num_classes` with values\n","        # representing the similarity of an image to each class.\n","\n","    def forward(self, image_features):\n","        # This function defines how the data flows through the model during forward propogation.\n","        # It's called automatically during training and inference.\n","\n","        return self.fc(image_features)\n","        # The output is a set of raw scores (logits) for each class.\n"],"metadata":{"id":"hiaZm425bZFU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step-by-Step Instructions for Re-training on New Data"],"metadata":{"id":"60WOdZ69lJdb"}},{"cell_type":"markdown","source":["This notebook is set up to work with any image classification dataset that follows this folder structure:\n","\n","```\n","data/\n","├── train/\n","│   ├── class_1/\n","│   │   ├── image_001.png\n","│   │   ├── ...\n","│   ├── class_2/\n","│   │   ├── image_001.png\n","│   │   ├── ...\n","│   └── ...\n","└── test/\n","```"],"metadata":{"id":"QyhhItFXlE5D"}},{"cell_type":"markdown","source":["If it does not, that is ok, but you will need to change the structure slightly."],"metadata":{"id":"6HCXHpADdpIX"}},{"cell_type":"markdown","source":["### **How to Run on New Data (Folder Structure)**"],"metadata":{"id":"tWc9dvdOmbxs"}},{"cell_type":"markdown","source":["1. Replace the Kaggle Dataset\n","\n","  Make sure the dataset has a train/ folder with subfolders for each class like the structure above\n","\n","2. Define Dataset Paths\n","Now set the path to your training folder. You may need to inspect the downloaded folder structure with os.listdir(path):\n","\n","  Then run:\n","\n","  ```\n","  dataset_root = os.path.join(path, 'YOUR_TRAIN_FOLDER_NAME_HERE')  # Update this!\n","\n","  val_root = \"/kaggle/working/YOUR_PROJECT_NAME/val\"  # Folder where val set will be saved\n","  ```\n","\n","3. Automatically Create a Validation Set (from training data)\n","\n","  There is a clear example of this below. This code splits 20% of each class into a new validation set. No need to touch anything in this part.\n","\n","4. Load the Data into PyTorch\n","\n","  No changes needed here — this automatically loads your training and validation data using torchvision.datasets.ImageFolder, which works with your folder structure.\n","\n","5. Automatically Generate Class Labels for CLIP\n","\n","  This code grabs all the class folder names and uses them to generate natural-language prompts like \"A photo of a husky\". You don’t need to manually type class names — it's all done for you.\n","\n","6. Define Test Set Path\n","\n","  Now, set the path to your test folder. You may need to inspect the new test folder structure just like you did with the training data.\n","\n","  ```\n","  test_root = os.path.join(path, 'YOUR_TEST_FOLDER_NAME_HERE')  # Update this!\n","  ```\n","  \n","7. Load the Test Data into PyTorch\n","\n","  Once the test path is set, this part will load the test data and create the corresponding DataLoader for evaluation\n","\n","8. Generate Class Names for the Test Set\n","\n","  This code retrieves all class names from the test dataset, which are used for generating the CLIP text features. This part is also automatic — no need to manually update class names.\n","\n","9. Generate Text Features for the Test Set\n","\n","  CLIP needs to know about your test classes to make predictions. This part creates text features for all classes in the test set, using the format: “A photo of a [class name].”\n","\n"],"metadata":{"id":"9YhgjDsed7UI"}},{"cell_type":"markdown","source":["**Summary - What to Edit:**\n","\n","* Kaggle dataset to use\n","\n","  ```\n","  kagglehub.dataset_download(...)\n","  ```\n","* Name of the train folder\n","  ```\n","  os.path.join(path, 'YOUR_FOLDER')\n","  ```\n","* Validation output folder name (somewhere on your working directory)\n","  ```\n","  val_root = \"/kaggle/working/...\"\n","  ```\n","* Test Set Folder Name Update the test folder name and path:\n","  ```\n","  test_root = os.path.join(path, 'YOUR_TEST_FOLDER_NAME_HERE')\n","  ```"],"metadata":{"id":"GDTLeAMXtpul"}},{"cell_type":"markdown","source":["### **If Your Dataset Uses a CSV File for Labels (Not Folder Structure)**"],"metadata":{"id":"ZGft7tJJmf0G"}},{"cell_type":"markdown","source":["Some data is instead stored like this:\n","\n","```\n","data/\n","├── train/\n","│   ├── image_001.png\n","│   ├── image_001.png\n","│   └── ...\n","├── test/\n","├── train.csv\n","└── test.csv\n","```\n","\n","with a csv file of labels\n","\n","```\n","filename,class\n","dog_001.jpg,husky\n","dog_002.jpg,beagle\n","dog_003.jpg,husky\n","````\n","\n","No problem! Here’s what you need to change:\n","\n","1. Load the CSV\n","\n","  ```\n","  csv_path = os.path.join(path, \"your_labels.csv\")  # update this!\n","  df = pd.read_csv(csv_path)\n","  ```\n","  Make sure your CSV has at least two columns: the image filename and the label.\n","\n","2. Update the Root Directory\n","\n","  This should point to the folder where the images live, usually called train or images:\n","\n","  ```\n","  image_folder = os.path.join(path, \"train\")\n","  ```\n","3. Create a Custom Dataset Class\n","\n","  Replace the ImageFolder logic with a custom dataset that uses your CSV and image folder:\n","\n","  ```\n","  class CSVDataset(Dataset):\n","      def __init__(self, dataframe, root_dir, transform=None):\n","          self.data = dataframe\n","          self.root_dir = root_dir\n","          self.transform = transform\n","\n","          # Map class names to numeric labels\n","          self.classes = sorted(self.data['class'].unique())\n","          self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n","          self.data['label'] = self.data['class'].map(self.class_to_idx)\n","\n","      def __len__(self):\n","          return len(self.data)\n","\n","      def __getitem__(self, idx):\n","          row = self.data.iloc[idx]\n","          img_path = os.path.join(self.root_dir, row['filename'])\n","          image = Image.open(img_path).convert(\"RGB\")\n","          label = row['label']\n","\n","          if self.transform:\n","              image = self.transform(image)\n","\n","          return image, label\n","    ```\n","4. Create Train/Val Splits with Custom Data Set\n","\n","  Once the custom dataset is ready, you can split the data into training and validation sets. This code will split 20% of each class into a validation set:\n","\n","  ```\n","  from sklearn.model_selection import train_test_split\n","\n","  train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['class'], random_state=42)\n","\n","  train_dataset = CSVDataset(train_df, root_dir=image_folder, transform=preprocess)\n","  val_dataset = CSVDataset(val_df, root_dir=image_folder, transform=preprocess)\n","\n","  train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","  val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","  ```\n","5. Update Class Names and Text Prompts\n","\n","  After loading the dataset, generate the text prompts for each class using the class column. This step is necessary to create the text features for CLIP, which will be used to train the model.\n","\n","  ```\n","  class_names = train_dataset.classes\n","\n","  with torch.no_grad():\n","      all_text_prompts = [f\"A photo of a {classname}\" for classname in class_names]\n","      tokenized_texts = clip.tokenize(all_text_prompts).to(device)\n","      text_features_all = model.encode_text(tokenized_texts)\n","      text_features_all = F.normalize(text_features_all, dim=-1).float()\n","    ```\n","6. Load the CSV File for the Test Set\n","\n","  Similar to the training set, you need to load the test.csv file that contains the image filenames and their corresponding labels:\n","\n","  ```\n","  csv_test_path = os.path.join(path, \"test.csv\")  # Update this with your test CSV file path!\n","  df_test = pd.read_csv(csv_test_path)\n","  ```\n","\n","  Ensure that the test CSV has at least two columns: filename and class.\n","\n","7. Update the Root Directory for Test Images\n","\n","  ```\n","  test_image_folder = os.path.join(path, \"test\")\n","  ```\n","\n","8. Use the Same Custom Dataset Class for the Test Set\n","\n","  ```\n","  test_dataset = CSVDataset(df_test, root_dir=test_image_folder, transform=preprocess)\n","  test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","  ```\n","\n","9. Generate Class Names and Text Prompts for CLIP (Test Set)\n","\n","  ```\n","  class_names_test = test_dataset.classes  # Retrieve the class names for the test set\n","\n","  with torch.no_grad():\n","      all_text_prompts_test = [f\"A photo of a {classname}\" for classname in class_names_test]\n","      tokenized_texts_test = clip.tokenize(all_text_prompts_test).to(device)\n","      text_features_all_test = model.encode_text(tokenized_texts_test)  # Shape: (num_classes, 512)\n","      text_features_all_test = F.normalize(text_features_all_test, dim=-1).float()  # Normalize the text features\n","  ```\n","\n","There is an example of this in the GTSRB file."],"metadata":{"id":"qWbRFAGDgsZU"}},{"cell_type":"markdown","source":["## **DATA: This is the part you edit**"],"metadata":{"id":"MqGkpdBUqLKo"}},{"cell_type":"markdown","source":["To run this re-training procedure this is the **only** part you want to edit. All necessary changes can be made here. Changes elsewhere may effect the model and make them difficult to compare."],"metadata":{"id":"WH6UfxxJmFXz"}},{"cell_type":"markdown","source":["### Ok, now we actually do this on a dogs data set"],"metadata":{"id":"hnUTNUhujS9c"}},{"cell_type":"markdown","source":["Load data"],"metadata":{"id":"sDqHKDmDxUJM"}},{"cell_type":"code","source":["# Download latest version\n","path = kagglehub.dataset_download(\"amirmakir/dogs-dataset\") # Change to your data set\n","print(\"Path to dataset files:\", path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zqOiUSclqNXA","outputId":"cd9bd8ce-1e69-4cb2-9e52-314fdfcb4892"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Path to dataset files: /kaggle/input/dogs-dataset\n"]}]},{"cell_type":"markdown","source":["Split out validation set"],"metadata":{"id":"LvZbwKwlxVLR"}},{"cell_type":"code","source":["dataset_root = os.path.join(path, 'dogs', 'train')  # Path to your \"train\" folder, change as needed\n","val_root = \"/kaggle/working/dogs/val\"  # Path to save the validation split (working directory), change as needed\n","# Define paths\n","\n","\n","if not os.path.exists(val_root):\n","    os.makedirs(val_root)\n","# Create the validation root folder if it doesn't exist\n","\n","\n","for class_name in os.listdir(dataset_root):\n","# Split data within each class folder\n","\n","    class_folder = os.path.join(dataset_root, class_name)\n","\n","    if os.path.isdir(class_folder):\n","\n","        image_files = [f for f in os.listdir(class_folder) if os.path.isfile(os.path.join(class_folder, f))]\n","        # Get list of image files in the class folder\n","\n","        train_files, val_files = train_test_split(image_files, test_size=0.2, random_state=42)\n","        # Split the images into training and validation sets\n","\n","        val_class_folder = os.path.join(val_root, class_name)\n","        if not os.path.exists(val_class_folder):\n","            os.makedirs(val_class_folder)\n","        # Create corresponding folders in the validation directory\n","\n","        for val_image in val_files:\n","            src = os.path.join(class_folder, val_image)\n","            dst = os.path.join(val_class_folder, val_image)\n","            shutil.copy(src, dst)  # Use copy instead of move\n","        # Copy validation images to the validation folder\n","\n","# After this, the validation set should be created in /kaggle/working/dogs/val\n"],"metadata":{"id":"21sGP7OGwU-M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Preprocess"],"metadata":{"id":"_O1QUnYtxZLR"}},{"cell_type":"code","source":["train_transform = preprocess\n","val_transform = preprocess\n","# Define the transformation for CLIP preprocessing (same as when we loaded the model)\n","# CLIP preprocess automatically resizes, normalizes, and converts to tensor\n","\n","train_dataset = datasets.ImageFolder(root=dataset_root, transform=train_transform)\n","val_dataset = datasets.ImageFolder(root=val_root, transform=val_transform)\n","# Create datasets for train and validation using ImageFolder\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","# Create DataLoaders for train and validation sets"],"metadata":{"id":"HOFSEO2hqXyo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Get classes"],"metadata":{"id":"xuTwmMQhxdoN"}},{"cell_type":"code","source":["class_names = train_dataset.classes\n","print(class_names)\n","# Extract class names from folders\n","\n","with torch.no_grad():\n","    all_text_prompts = [f\"A photo of a {classname}\" for classname in class_names]\n","    tokenized_texts = clip.tokenize(all_text_prompts).to(device)\n","    text_features_all = model.encode_text(tokenized_texts)\n","    text_features_all = F.normalize(text_features_all, dim=-1).float()  # <- add .float() here\n","# Update class names with text prompt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"My0rOfwSrP6h","outputId":"a679bda5-88dc-4c72-9fc6-59d75cddef62"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Afghan_hound', 'Blenheim_spaniel', 'Chihuahua', 'Japanese_spaniel', 'Maltese_dog', 'Pekinese', 'Rhodesian_ridgeback', 'Shih_Tzu', 'papillon', 'toy_terrier']\n"]}]},{"cell_type":"markdown","source":["Get test set set up for later"],"metadata":{"id":"jPTip_n7xhZV"}},{"cell_type":"code","source":["# Paths\n","test_root = os.path.join(path, 'dogs', 'test') # Change this for your own data\n","\n","# Load test set\n","test_dataset = datasets.ImageFolder(root=test_root, transform=preprocess)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","# Get all class names\n","class_names_test = test_dataset.classes\n","print(\"Class names:\", class_names_test)\n","\n","# Generate text features for all classes once\n","with torch.no_grad():\n","    all_texts = [f\"A photo of a {classname}\" for classname in class_names_test] # Feel free to change the prompt is desired\n","    tokenized_texts = clip.tokenize(all_texts).to(device)\n","    text_features_all = model.encode_text(tokenized_texts)  # Shape: (num_classes, 512)"],"metadata":{"id":"jvfqV9oxnhsW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8be6fdf7-416f-49b3-a0b1-0065c3fd058a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Class names: ['Afghan_hound', 'Blenheim_spaniel', 'Chihuahua', 'Japanese_spaniel', 'Maltese_dog', 'Pekinese', 'Rhodesian_ridgeback', 'Shih_Tzu', 'papillon', 'toy_terrier']\n"]}]},{"cell_type":"markdown","source":["## **OK, STOP EDITING HERE**\n","\n","The rest of this file should work just fine without edits if you didn't change any variable names"],"metadata":{"id":"qjcXBTSjki5s"}},{"cell_type":"markdown","source":["## Let's Retrain"],"metadata":{"id":"jYoC_6-UYJ4V"}},{"cell_type":"code","source":["classifier = LinearClassifier(input_dim=512, num_classes=len(class_names)).to(device)\n","# Initializes the classifier we defined earlier\n","\n","optimizer = torch.optim.AdamW([\n","    {\"params\": model.visual.parameters(), \"lr\": 1e-6},\n","    {\"params\": classifier.parameters(), \"lr\": 1e-4}\n","], weight_decay=1e-4)\n","# We're training two parts:\n","# 1) model.visual: The vision encoder from CLIP — we fine-tune it very gently using a small learning rate (1e-6)\n","# 2) classifier: Our new linear layer — it starts from scratch, so we train it more aggressively (1e-4)\n","# AdamW is a common optimizer\n","\n","criterion = nn.CrossEntropyLoss()\n","# Cross-entropy compares the predicted scores (logits) against the true label and penalizes wrong guesses.\n","\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n","# This slowly reduces the learning rate over time in a smooth cosine curve\n","# this is a common trick to make training more stable and avoid overshooting the minimum loss.\n","\n","\n","num_epochs = 10\n","# how many times we loop through the whole dataset\n","\n","best_val_acc = 0\n","# keeps track of the best accuracy we've seen so far\n","\n","patience = 3\n","# For early stopping — we stop training if validation accuracy doesn’t improve for 3 straight epochs\n","# This trains more efficiently and prevents overfitting\n","\n","epochs_no_improve = 0\n","# how many times we've failed to beat our best accuracy\n","\n","for epoch in range(num_epochs):\n","\n","    classifier.train()\n","    # classifier.train() puts the model in training mode\n","\n","    total_loss, correct, total = 0, 0, 0\n","\n","    # Training ##################################################\n","\n","    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Train)\"):\n","\n","        images, labels = images.to(device), labels.to(device)\n","        image_features = model.encode_image(images).float()\n","        # Use CLIP’s vision model to encode the images into 512-dimension feature vectors\n","        image_features = F.normalize(image_features, dim=-1)\n","        # Normalize them (unit length) so comparisons (dot products) behave like cosine similarity\n","\n","        with torch.no_grad():\n","            clip_logits = image_features @ text_features_all.T  # (B, num_classes)\n","        # Dot product between image and text features. Gives similarity scores\n","        # (logits) between each image and all class names.\n","\n","        classifier_logits = classifier(image_features)\n","        # our classifier’s own guess — based on its trained weights\n","\n","        clip_logits = clip_logits / clip_logits.norm(dim=-1, keepdim=True)\n","        classifier_logits = classifier_logits / classifier_logits.norm(dim=-1, keepdim=True)\n","        # Normalize - ensures the same scale\n","\n","        blended_logits = 0.5 * classifier_logits + 0.5 * clip_logits\n","        # average the scores from CLIP and our linear classifier\n","\n","        loss = criterion(blended_logits, labels)\n","        # Calculate the loss from the blended prediction\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        # Clear old gradients, backpropagate new ones, and take an optimizer step\n","\n","        total_loss += loss.item()\n","        correct += (blended_logits.argmax(dim=1) == labels).sum().item()\n","        total += labels.size(0)\n","        # Count how many predictions were correct and update total loss and accuracy\n","\n","    train_acc = 100 * correct / total\n","    print(f\"Epoch {epoch+1}: Train Loss = {total_loss:.4f}, Train Acc = {train_acc:.2f}%\")\n","\n","    # Validation ################################################\n","\n","    classifier.eval()\n","    # Switch model to evaluation mode\n","\n","    correct, total = 0, 0\n","\n","    with torch.no_grad():\n","        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Val)\"):\n","            images, labels = images.to(device), labels.to(device)\n","            image_features = model.encode_image(images).float()\n","            image_features = F.normalize(image_features, dim=-1)\n","\n","            with torch.no_grad():\n","                clip_logits = image_features @ text_features_all.T\n","            classifier_logits = classifier(image_features)\n","            clip_logits = clip_logits / clip_logits.norm(dim=-1, keepdim=True)\n","            classifier_logits = classifier_logits / classifier_logits.norm(dim=-1, keepdim=True)\n","            logits = 0.5 * classifier_logits + 0.5 * clip_logits\n","            correct += (logits.argmax(dim=1) == labels).sum().item()\n","            total += labels.size(0)\n","\n","    val_acc = 100 * correct / total\n","    print(f\"Epoch {epoch+1}: Val Acc = {val_acc:.2f}%\")\n","    # Count correct predictions to compute validation accuracy\n","\n","    # Early Stopping #############################################\n","\n","    if val_acc > best_val_acc:\n","        best_val_acc = val_acc\n","        epochs_no_improve = 0\n","        torch.save(classifier.state_dict(), 'best_linear_classifier.pth')\n","        print(\"Improved validation accuracy. Saved model.\")\n","    # If we beat our best validation accuracy, save the model\n","\n","    else:\n","        epochs_no_improve += 1\n","        if epochs_no_improve >= patience:\n","            print(\"Early stopping.\")\n","            break\n","    # If we’ve gone patience epochs with no improvement, stop training early\n","\n","    scheduler.step()\n","    # Move along the cosine schedule — lower the learning rate a bit\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6O2clmHwvIkl","outputId":"3e2247c2-69e8-4913-9b80-cc36b0c1bd88"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1/10 (Train): 100%|██████████| 51/51 [00:17<00:00,  2.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Train Loss = 112.3489, Train Acc = 33.65%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/10 (Val): 100%|██████████| 11/11 [00:01<00:00,  5.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Val Acc = 68.52%\n","Improved validation accuracy. Saved model.\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/10 (Train): 100%|██████████| 51/51 [00:13<00:00,  3.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Train Loss = 100.9878, Train Acc = 76.72%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/10 (Val): 100%|██████████| 11/11 [00:01<00:00,  6.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Val Acc = 88.58%\n","Improved validation accuracy. Saved model.\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/10 (Train): 100%|██████████| 51/51 [00:12<00:00,  3.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3: Train Loss = 97.3580, Train Acc = 90.70%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/10 (Val): 100%|██████████| 11/11 [00:01<00:00,  6.15it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3: Val Acc = 91.98%\n","Improved validation accuracy. Saved model.\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/10 (Train): 100%|██████████| 51/51 [00:12<00:00,  3.95it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4: Train Loss = 95.6824, Train Acc = 94.19%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/10 (Val): 100%|██████████| 11/11 [00:01<00:00,  6.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4: Val Acc = 95.37%\n","Improved validation accuracy. Saved model.\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/10 (Train): 100%|██████████| 51/51 [00:12<00:00,  3.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5: Train Loss = 94.5789, Train Acc = 96.32%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/10 (Val): 100%|██████████| 11/11 [00:01<00:00,  6.06it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5: Val Acc = 96.91%\n","Improved validation accuracy. Saved model.\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/10 (Train): 100%|██████████| 51/51 [00:12<00:00,  4.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6: Train Loss = 93.9600, Train Acc = 97.19%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/10 (Val): 100%|██████████| 11/11 [00:01<00:00,  6.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6: Val Acc = 97.22%\n","Improved validation accuracy. Saved model.\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/10 (Train): 100%|██████████| 51/51 [00:12<00:00,  3.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7: Train Loss = 93.3998, Train Acc = 97.69%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/10 (Val): 100%|██████████| 11/11 [00:01<00:00,  6.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7: Val Acc = 98.15%\n","Improved validation accuracy. Saved model.\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/10 (Train): 100%|██████████| 51/51 [00:12<00:00,  3.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8: Train Loss = 93.1269, Train Acc = 98.13%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/10 (Val): 100%|██████████| 11/11 [00:02<00:00,  5.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8: Val Acc = 98.77%\n","Improved validation accuracy. Saved model.\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/10 (Train): 100%|██████████| 51/51 [00:12<00:00,  3.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9: Train Loss = 92.9793, Train Acc = 98.19%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/10 (Val): 100%|██████████| 11/11 [00:01<00:00,  5.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9: Val Acc = 99.07%\n","Improved validation accuracy. Saved model.\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/10 (Train): 100%|██████████| 51/51 [00:12<00:00,  3.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10: Train Loss = 92.9358, Train Acc = 98.31%\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/10 (Val): 100%|██████████| 11/11 [00:01<00:00,  6.05it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 10: Val Acc = 99.07%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["## Compute Accuracy with Newly Trained Model"],"metadata":{"id":"wDH6f6RZ1HGc"}},{"cell_type":"code","source":["def compute_topk_accuracy(logits, labels, topk=(1, 3, 5)):\n","    max_k = max(topk)\n","    batch_size = labels.size(0)\n","\n","    _, pred = logits.topk(max_k, dim=1, largest=True, sorted=True)\n","    pred = pred.t()\n","    correct = pred.eq(labels.view(1, -1).expand_as(pred))\n","\n","    topk_accs = {}\n","    for k in topk:\n","        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n","        topk_accs[f\"top{k}\"] = (correct_k / batch_size).item() * 100.0\n","\n","    return topk_accs\n","\n","# Evaluate fine-tuned classifier\n","classifier.eval()\n","top1_total, top3_total, top5_total, total_samples = 0, 0, 0, 0\n","\n","with torch.no_grad():\n","    for images, labels in tqdm(test_loader, desc=\"Evaluating Fine-tuned Classifier\"):\n","        images, labels = images.to(device), labels.to(device)\n","        image_features = model.encode_image(images).float()\n","        image_features = F.normalize(image_features, dim=-1)\n","\n","        logits = classifier(image_features)\n","        accs = compute_topk_accuracy(logits, labels)\n","\n","        top1_total += accs['top1'] * images.size(0)\n","        top3_total += accs['top3'] * images.size(0)\n","        top5_total += accs['top5'] * images.size(0)\n","        total_samples += images.size(0)\n","\n","print(f\"\\nFine-tuned Classifier Accuracy:\")\n","print(f\"Top-1: {top1_total / total_samples:.2f}%\")\n","print(f\"Top-3: {top3_total / total_samples:.2f}%\")\n","print(f\"Top-5: {top5_total / total_samples:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PqmSJM0h1bqP","outputId":"075b4fd2-2e8d-4bae-c318-f1442d8be915"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Evaluating Fine-tuned Classifier: 100%|██████████| 10/10 [00:03<00:00,  3.09it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Fine-tuned Classifier Accuracy:\n","Top-1: 88.64%\n","Top-3: 98.11%\n","Top-5: 99.37%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["## Compare To Zero Shot Accuracy"],"metadata":{"id":"owv0DDDIxp5s"}},{"cell_type":"code","source":["original_model = clip.load(\"ViT-B/32\", device=device)[0].float().eval()\n","with torch.no_grad():\n","    tokenized_texts = clip.tokenize([f\"A photo of a {classname}\" for classname in class_names_test]).to(device)\n","    text_features_all = original_model.encode_text(tokenized_texts)\n","    text_features_all = F.normalize(text_features_all, dim=-1).float()\n","\n","def compute_zero_shot_topk_accuracy(model, image_loader, text_features_all, device):\n","    model.eval()\n","    text_features_all = F.normalize(text_features_all, dim=-1)\n","\n","    top1_total, top3_total, top5_total, total_samples = 0, 0, 0, 0\n","\n","    with torch.no_grad():\n","        for images, labels in tqdm(image_loader, desc=\"Evaluating Zero-Shot CLIP\"):\n","            images, labels = images.to(device), labels.to(device)\n","            image_features = model.encode_image(images).float()\n","            image_features = F.normalize(image_features, dim=-1)\n","\n","            logits = image_features @ text_features_all.T\n","            accs = compute_topk_accuracy(logits, labels)\n","\n","            top1_total += accs['top1'] * images.size(0)\n","            top3_total += accs['top3'] * images.size(0)\n","            top5_total += accs['top5'] * images.size(0)\n","            total_samples += images.size(0)\n","\n","    return {\n","        'top1': top1_total / total_samples,\n","        'top3': top3_total / total_samples,\n","        'top5': top5_total / total_samples,\n","    }\n","\n","# Run zero-shot evaluation\n","zero_shot_results = compute_zero_shot_topk_accuracy(original_model, test_loader, text_features_all, device)\n","\n","print(\"\\nZero-Shot CLIP Accuracy:\")\n","print(f\"Top-1: {zero_shot_results['top1']:.2f}%\")\n","print(f\"Top-3: {zero_shot_results['top3']:.2f}%\")\n","print(f\"Top-5: {zero_shot_results['top5']:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SG_A74iYEQFl","outputId":"fecbec1f-6cd7-4339-a6b0-de7b56f1b955"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Evaluating Zero-Shot CLIP: 100%|██████████| 10/10 [00:02<00:00,  4.84it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Zero-Shot CLIP Accuracy:\n","Top-1: 74.45%\n","Top-3: 91.17%\n","Top-5: 97.48%\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}