# Retrain_CLIP

This project, sponsored by a special topics data science course (MA 591) at NCSU in collaboration with Sandia National Laboratories, aimed to enhance our understanding of trustworthy generative AI and its practical applications. Our small group of four, under faculty supervision, investigated OpenAIâ€™s Contrastive Language Image Pre-Training (CLIP) model, which is used for extracting and editing image information. We validated existing claims about the model and identified areas for improvement. To address known weaknesses and significantly boost performance, we experimented with retraining the original CLIP model through fine-tuning, transfer learning, and ensemble learning.

RETRAIN_BASE_MODEL.ipynb contains the retraining framework and a discussion on implementation. 

The testing folder contains the scripts we wrote to compare accuracy results with the original CLIP model versus our retrained model.  

GroupPaperCLIP.pdf is our comprehensive written report that documents our completed project. 
